Recent papers for Yossi Gil:

2020: Ties between Parametrically Polymorphic Type Systems and Finite Control Automata
Abstract: We present a correspondence and bisimulation between variants of parametrically polymorphic type systems and variants of finite control automata, such as FSA, PDA, tree automata and Turing machine. Within this correspondence we show that two recent celebrated results on automatic generation of fluent API are optimal in certain senses, present new results on the studied type systems, formulate open problems, and present potential software engineering applications, other than fluent API generation, which may benefit from judicious use of type theory.

2019: Fling - A Fluent API Generator
Abstract: None

2019: Fling - A Fluent API Generator (Artifact)
Abstract: The first general and practical solution of the fluent API problem is presented. We give an algorithm that given a deterministic context free language (equivalently, LR(k), k ≥ 0 language) encodes it in an unbounded parametric polymorphism type system employing only a polynomial number of types. The theoretical result is employed in an actual tool Fling– a fluent API compiler-compiler in the style of YACC, tailored for embedding DSLs in Java. 2012 ACM Subject Classification Software and its engineering → General programming languages; Software and its engineering → Domain specific languages

2019: A Nano-Pattern Language for Java
Abstract: None

2017: Pluggable Controllers and Nano-Patterns
Abstract: This paper raises the idea of giving end users the ability to modify and extend the control flow constructs (if, while, etc.) of the underlying programming language, just as they can modify and extend the library standard implementation of function printf and class String. Pluggable Controllers are means for modular design of control constructors, e.g., if, while, do, switch, and operators such as short circuit conjunction (&&) and the “?.” operator of the Swift programming language. We propose a modular, pluggable controllers based, design of a language. In this design there are control constructors which are core, augmented by a standard library of control constructors, which just like all standard libraries, is extensible and replaceable. The control constructors standard library can then follow a course of evolution that is less coupled with that of the main language, where a library release does not mandate new language release. At the same time, the library could be extended by individuals, corporate and communities to implement more or less idiosyncratic Nano-Patterns. We demonstrate the imposition of pluggable control constructors on Java by employing Lola—a Turing-complete and programming language independent code preprocessor.

2017: Code Spartanization: one rational approach for resolving religious style wars
Abstract: Spartan programming is a coding style which tries to minimize the elements of code, like in a laconic speech. In the context of code, the minimized elements of speech include lines, characters, arguments, nesting use of ifs and whiles, etc. The style is achieved by the process of repeated application of code transformation techniques, or refactoring operations, drawn from the spartan toolbox of tippers. Each tipper improves at least one of the code size metrics, without degrading any of the others. We present the unique look of spartan code, and the process of achieving it, including the three main kinds of tippers: structural, nominal, and modular. We do not make the case for the spartan style here, leaving the readers to find beauty or savageness in it. The evaluation part of this work gives evidence that the application of structural tippers contributes to the naturalness of software.

2017: On the correlation between size and metric validity
Abstract: None

2017: Fitting long‐tailed distribution to empirical data
Abstract: Power laws can fit a variety of distributions coming from real data, so a systematic approach to the measurement of the accuracy of fitting algorithms is essential. We discuss the limits of the analysis of empirical fat‐tailed distributions, which can describe a variety of evolving systems, both natural and man‐made. An algorithm to fit fat‐tailed distributions is presented and tested against samplings of the power law, the Yule, the log‐normal, and Weibull distributions. We compute the parameters defining the shape of each distribution and test the results against simulations. We compare our method with another state‐of‐the‐art technique to estimate the parameters of empirical distributions. The accuracy of the estimations is discussed, and we conclude that our method based on a weighted iterated χ2 test performs better than the other. Our algorithm is general and can be applied to any numerical dataset.

2017: The Spartanizer: Massive automatic refactoring
Abstract: The Spartanizer is an eclipse plugin featuring over one hundred and fifty refactoring techniques, all aimed at reducing various size complexity of the code, without changing its design, i.e., inheritance relations, modular structure, etc. Typical use case of the Spartanizer is in an automatic mode: refactoring operations are successively selected and applied by the tool, until the code is reshaped in spartan style (a frugal coding style minimizing the use of characters, variables, tokens, etc.). The Spartanizer demonstrates the potential of automatic refactoring: tens of thousands of transformations are applied in matter of seconds, chains of dependent applications of transformations with tens of operations in them, significant impact on code size, and extent reaching almost every line of code, even of professional libraries.

2017: Syntactic Zoom-Out / Zoom-In Code with the Athenizer
Abstract: Care and great e.ort are often taken to dress program code of libraries, just as model implementations, in its most presentable form, which includes adherence to strict coding standards, careful selection of identifiers, avoiding unnecessary constructs, etc. However, a presentable dress is not a janitor's uniform and is often inferior to the more lax working outfit.The spartanizer is a tool that brings Java code into a canonical, short form. Trying to say the most with the fewest words. In contrast, the athenizer is a tool that expands the code, placing it in a more maintainable form, using plenty of auxiliary variables, many potential locations for breakpoints and for change.The tool reported on here allows developers to interactively use their joystick and its buttons for code navigation, and in particular for zooming-in into the code (athenizing) and zooming-out of it (spartanizing).

2016: Formal Language Recognition with the Java Type Checker
Abstract: This paper is a theoretical study of a practical problem: the 
automatic generation of Java Fluent APIs from their specification. We explain why the problem's core lies with the expressive power of Java generics. Our main result is that automatic generation is possible whenever the specification is an instance of the set of deterministic context-free languages, a set which contains most "practical" languages. Other contributions include a collection of techniques and idioms of the limited meta-programming possible with Java generics, and an empirical measurement demonstrating that the runtime of the "javac" compiler of Java may be exponential in the program's length, even for programs composed of a handful of lines and which do not rely on overly complex use of generics.

2016: When do Software Complexity Metrics Mean Nothing? - When Examined out of Context
Abstract: This paper places its attention on a familiar phenomena: that code metrics such as lines of code are extremely context dependent and their distribution differs from project to project. We apply visual inspection, as well as statistical reasoning and testing, to show that such metric values are so sensitive to context, that their measurement in one project offers little prediction regarding their measurement in another project. On the positive side, we show that context bias can be neutralized, at least for the majority of metrics that we considered, by what we call Log Normal Standardization (LNS). Concretely, the LNS transformation is obtained by shifting (by subtracting the mean) and scaling (by dividing by the standard deviation) of the log of a metric value. Thus, we conclude that the LNS-transformed-, are to be preferred over the plain-, values of metrics, especially in comparing modules from different projects. Conversely, the LNS-transformation suggests that the “context bias” of a software project with respect to a specific metric can be summarized with two numbers: the mean of the logarithm of the metric value, and its standard deviation.

2016: When do Software Complexity Metrics Mean Nothing? – When Examined out of Context.
Abstract: None

2016: Structured gotos are (Slightly) harmful
Abstract: We take up the questions of if and how "structured goto" statements impact defect proneness, and of which what concept of size yields a superior metric for defect prediction. We count goto-like unstructured jumps, alongside method size and compressed method size, as software engineering metrics, and examine the evolution of 26 open-source code corpora in relation to those metrics. We employ five different measures of defectiveness and development effort. We measure the statistical quality of our metrics as predictors of our defect measurements. We show that the number of unstructured jumps is a predictor of defects, routine maintenance and two other metrics of software development effort. The correlation between unstructured jumps and development effort is positive, and it remains so even after accounting for the effect of code size. We also show that the number of unstructured jumps is superior to code size, both compressed and uncompressed, in its predictive power of accumulated defects.

2015: On Incomplete Bug Fixes in Eclipse and Programmers' Intuition on These
Abstract: Recent studies indicate that multiple patches to software are found in a hefty portion of resolved bugs. It is also known that bugs that require multiple patches take longer to resolve, that their severity tends to be higher than the average and that they induce programmers to engage more in bug discussions. This work is concerned with the ability of programmers to predict a bug will be of this sort, and in particular that it may require future patches and greater refixing effort at the time the bug is fixed at the first time. A mathematical model is developed for a retrospective analysis of bugs maintenance history. In this model we compute the impact of an array of bug properties on the likelihood that a specific bug is chosen, among all open bugs, to receive its first fix. The studies we conduct on a sizable portion of the history of the Eclipse code base indicate that programmers tend to attend first to bugs which are easier to fix. The results further suggest that some of the criteria that programmers apply (probably unknowingly) to determine whether a bug is easy to fix, is the number of future patches it would require, and the amount of work involved in these patches. This is despite the fact that this information is not supposed to be available to the programmers at the time the first fix is made. It is anticipated that the method of analysis introduced in this work would have other applications in software engineering, but also outside of computer science.

2013: Proceedings of the 7th ACM workshop on Virtual machines and intermediate languages
Abstract: An increasing number of high-level programming language implementations is realized using standard virtual machines. Recent examples of this trend include the Clojure (Lisp) and Potato (Squeak Smalltalk) projects, which are implemented on top of the Java Virtual Machine (JVM); and also F# (ML) and IronPython, which target the .NET CLR. Making diverse languages--possibly even adopting different paradigms--available on a robust and efficient common platform leverages language interoperability. 
 
Vendors of standard virtual machine implementations have started to adopt extensions supporting this trend from the run-time environment side. For instance, the Sun standard JVM will include the invokedynamic instruction, which will facilitate a simpler implementation of dynamic programming languages on the JVM. 
 
The observation that many language constructs are supported in library code, or through code transformations leading to over-generalized results, has led to efforts to make the core mechanisms of certain programming paradigms available at the level of the virtual machine implementation. Thus, dedicated support for language constructs enables sophisticated optimization by direct access to the running system. This approach has been adopted by several projects aiming at providing support for aspect-oriented programming or dynamic dispatch in general-purpose virtual machines (Steamloom, Nu, ALIA4J). 
 
The main themes of this workshop are to investigate which programming language mechanisms are worthwhile candidates for integration with the run-time environment, how said mechanisms can be declaratively (and re-usably) expressed at the intermediate language level (e.g., in bytecode), how their implementations can be optimized, and how virtual machine architectures might be shaped to facilitate such implementation efforts. Possible candidates for investigation include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, paradigm-specific abstractions, and combinations of paradigms. 
 
The areas of interest include, but are not limited to, compilation-based and interpreter-based virtual machines as well as intermediate-language designs with better support for investigated language mechanisms, compilation techniques from high-level languages to enhanced intermediate languages as well as native machine code, optimization strategies for reduction of run-time overhead due to either compilation or interpretation, advanced caching and memory management schemes in support of the mechanisms, and additional virtual machine components required to manage them.

2012: A Bijective String Sorting Transform
Abstract: Given a string of characters, the Burrows-Wheeler Transform rearranges the characters in it so as to produce another string of the same length which is more amenable to compression techniques such as move to front, run-length encoding, and entropy encoders. We present a variant of the transform which gives rise to similar or better compression value, but, unlike the original, the transform we present is bijective, in that the inverse transformation exists for all strings. Our experiments indicate that using our variant of the transform gives rise to better compression ratio than the original Burrows-Wheeler transform. We also show that both the transform and its inverse can be computed in linear time and consuming linear storage.

2012: Empirical Confirmation (and Refutation) of Presumptions on Software
Abstract: Code metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some "believable" presumptions on the behavior of software and metrics measured for this software. Among those are the reliability presumption implicit in the application of any code metric, and the presumption that the magnitude of change in a software artifact is correlated with changes to its version number. 
Putting a suite of 36 metrics to the trial, we confirm most of the presumptions. Unexpectedly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments.

2012: An empirical investigation of changes in some software properties over time
Abstract: Software metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some plausible assumptions on the behavior of software and metrics measured for this software in retrospective on its versions evolution history. Among those are the reliability assumption implicit in the application of any code metric, and the assumption that the magnitude of change, i.e., increase or decrease of its size, in a software artifact is correlated with changes to its version number. Putting a suite of 36 metrics to the trial, we confirm most of the assumptions on a large repository of software artifacts. Surprisingly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments.

2012: Meta to the Rescue—Augmented Metamodels for Better Deployment of Pre-Packaged Applications
Abstract: Recent estimates indicate that more than half the software market belongs to enterprise applications. One of the greatest challenges in these is in conducting the complex process of adaptation of pre-packaged applications, such as Oracle or SAP, to the organization needs. Although very detailed, structured and well documented methods govern this process, the consulting team implementing the method must spend much manual effort in making sure that the guidelines of the method are followed as intended by the method author. The problem is exacerbated by the diversity of skills and roles of team members, and the many sorts of communications of collaboration that methods prescribe. By enhancing the metamodel in which the methods are defined, we automatically produce a CASE tool (so to speak) for the applications of these methods. Our results are successfully employed in a number of large, ongoing projects with demonstrable, non-meager saving.

