Recent papers for Cassuto Yuval:

2024: Traffic-Aware Merkle Trees for Shortening Blockchain Transaction Proofs
Abstract: Merkle trees play a crucial role in blockchain networks in organizing network state. They allow proving a particular value of an entry in the state to a node that maintains only the root of the Merkle trees, a hash-based signature computed over the data in a hierarchical manner. Verification of particular state entries is crucial in reaching a consensus on the execution of a block where state information is required in the processing of its transactions. For instance, a payment transaction should be based on the balance of the two involved accounts. The proof length affects the network communication and is typically logarithmic in the state size. In this paper, we take advantage of typical transaction characteristics for better organizing Merkle trees to improve blockchain network performance. We focus on the common transaction processing where Merkle proofs are jointly provided for multiple accounts. We first provide lower bounds for the communication cost that are based on the distribution of accounts involved in the transactions. We then describe algorithms that consider traffic patterns for significantly reducing it. The algorithms are inspired by various coding methods such as Huffman coding, partition and weight balancing. We also generalize our approach towards the encoding of smart contract transactions that involve an arbitrary number of accounts. Likewise, we rely on real blockchain data to show the savings allowed by our approach. The experimental evaluation is based on transactions from the Ethereum network and demonstrates cost reduction for both payment transactions and smart contract transactions.

2024: Graph Codes for Dual-Parameter Barrier Channels
Abstract: A ternary barrier channel is a non-symmetric error model defined previously to address emerging applications. Conveniently, it admits a code-construction method comprising two binary constituent codes. In this paper we propose a decoding algorithm that decodes the two constituent codes jointly by message passing on a graph representing the two codes' parity-check constraints. The messages exchanged by the algorithm are likelihoods calculated from incoming messages, and they are derived in the paper based on the exact dependence between the binary values of the two codewords. Simulation results demonstrate that the proposed decoder has superior error-rate performance compared to prior decoding approaches.

2024: Robust Regression with Ensembles Communicating over Noisy Channels
Abstract: As machine-learning models grow in size, their implementation requirements cannot be met by a single computer system. This observation motivates distributed settings, in which intermediate computations are performed across a network of processing units, while the central node only aggregates their outputs. However, distributing inference tasks across low-precision or faulty edge devices, operating over a network of noisy communication channels, gives rise to serious reliability challenges. We study the problem of an ensemble of devices, implementing regression algorithms, that communicate through additive noisy channels in order to collaboratively perform a joint regression task. We define the problem formally, and develop methods for optimizing the aggregation coefficients for the parameters of the noise in the channels, which can potentially be correlated. Our results apply to the leading state-of-the-art ensemble regression methods: bagging and gradient boosting. We demonstrate the effectiveness of our algorithms on both synthetic and real-world datasets.

2023: Ensemble Classification With Noisy Real-Valued Base Functions
Abstract: In data-intensive applications, it is advantageous to perform partial processing close to the data, and communicate intermediate results to a central processor, instead of the data itself. When the communication or computation medium is noisy, the resulting degradation in computation quality at the central processor must be mitigated. We study this problem for the setup of binary classification performed by an ensemble of base functions communicating real-valued confidence levels. We propose a noise-mitigation solution that optimizes the transmission gains and aggregation coefficients of the base functions. Toward that, we formulate a post-training gradient-based optimization algorithm that minimizes the error probability given the training dataset and the noise parameters. We further derive lower and upper bounds on the optimized error probability, and show empirical results that demonstrate the enhanced performance achieved by our approach on real data.

2023: A Unified, SNR-Aware SC-LDPC Code Design With Applications to Magnetic Recording
Abstract: Spatially coupled (SC)-low-density parity-check (LDPC) codes are known to have outstanding error-correction performance and low decoding latency, which make them an excellent choice for high-density magnetic recording (MR) technologies. Whereas previous works on LDPC and SC-LDPC codes mostly take either an asymptotic or a finite-length design approach, we propose a unified framework for jointly optimizing the codes’ thresholds and cycle counts to address both regimes. We focus on circulant-based (CB) SC-LDPC code family as a representative, high-performance exemplar of structured SC-LDPC codes. The framework is based on efficient traversal and pruning of the code search space, building on the fact that the performance of a CB SC-LDPC code depends on some characteristics of the code’s partitioning matrix, which by itself is much smaller than the code’s full parity-check matrix. We then propose an algorithm that traverses all non-equivalent partitioning matrices and outputs a list of codes, each offering an attractive point on the trade-off between asymptotic and finite-length performance. Our simulations show that our framework results in SC-LDPC codes that outperform the state-of-the-art constructions, over both additive white Gaussian noise (AWGN) and partial response (PR) channel models, and that it offers the flexibility to choose low-signal-to-noise ratio (SNR), high-SNR, or in- between SNR region considering system requirements, e.g., that of the MR device.

2023: Construction and Decoding of Codes over the Dual-Parameter Barrier Error Model
Abstract: Barrier-error channels have been suggested as a model for non-binary channels that are milder than symmetric-error channels. Such channels are motivated by practical applications in data storage and communications. The barrier-error model allows errors only to (downward) and from (upward) a specific symbol within the alphabet. We study a generalization of prior barrier-error models that considers different numbers of downward and upward errors. The results of this paper include a sufficient condition for error correction, code-size upper and lower bounds, a code construction decomposing to just two constituent symmetric-error codes (prior ones used many), and a decoding algorithm that decodes the two codes jointly. The decoder is shown empirically to achieve better block error rates when compared to previous algorithms.

2023: ECC-Map: A Resilient Wear-Leveled Memory-Device Architecture with Low Mapping Overhead
Abstract: New non-volatile memory technologies show great promise for extending the memory hierarchy, but have limited endurance that needs to be mitigated toward their reliable use closer to the processor. Wear leveling is a common technique for prolonging the life of endurance-limited memory, where existing wear-leveling approaches either employ costly full-indirection mapping between logical and physical addresses, or choose simple mappings that cannot cope with extremely unbalanced write workloads. In this work, we propose ECC-Map, a new wear-leveling device architecture that can level even the most unbalanced and adversarial workloads, while enjoying low mapping complexity compared to full indirection. Its key idea is using a family of efficiently computable mapping functions allowing to selectively remap heavily written addresses, while controlling the mapping costs by limiting the number of functions used at any given time. ECC-Map is evaluated on common synthetic workloads, and is shown to significantly outperform existing wear-leveling architectures. The advantage of ECC-Map grows with the device’s size-to-endurance ratio, a parameter that is expected to grow in the scaling trend of growing capacities and shrinking reliabilities.

2023: Distributed Boosting Classification Over Noisy Communication Channels
Abstract: We address the design of inference-oriented communication systems where multiple transmitters send partial inference values through noisy communication channels, and the receiver aggregates these channel outputs to obtain a reliable final inference. Since large data items are replaced by compact inference values, these systems lead to significant savings of communication resources. In particular, we present a principled framework to optimize communication-resource allocation for distributed boosting classifiers. Boosting classification algorithms make a final decision via a weighted vote from the outputs of multiple base classifiers. Since these base classifiers transmit their partial inference values over noisy channels, communication errors would degrade the final classification accuracy. We formulate communication resource allocation problems to maximize the final classification accuracy by taking into account the importance of base classifiers and the resource budget. To solve these problems rigorously, we formulate convex optimization problems to optimize: 1) transmit-power allocations and 2) transmit-rate allocations. This framework departs from classical communication-systems optimizations in seeking to maximize the classification accuracy rather than the reliability of the individual communicated bits. Results from numerical experiments demonstrate the benefits of our approach.

2022: Mitigating Noise in Ensemble Classification with Real-Valued Base Functions
Abstract: In data-intensive applications, it is advantageous to perform some partial processing close to the data, and communicate to a central processor the partial results instead of the data itself. When the communication medium is noisy, one must mitigate the resulting degradation in computation quality. We study this problem for the setup of binary classification performed by an ensemble of functions communicating real-valued confidence levels. We propose a noise-mitigation solution that works by optimizing the aggregation coefficients at the central processor. Toward that, we formulate a post-training gradient algorithm that minimizes the error probability given the dataset and the noise parameters. We further derive lower and upper bounds on the optimized error probability, and show empirical results that demonstrate the enhanced performance achieved by our scheme on real data.

2022: Genomic Compression With Read Alignment at the Decoder
Abstract: We propose a new compression scheme for genomic data given as sequence fragments called reads. The scheme uses a reference genome at the decoder side only, freeing the encoder from the burdens of storing references and performing computationally costly alignment operations. The main ingredient of the scheme is a multi-layer code construction, delivering to the decoder sufficient information to align the reads, correct their differences from the reference, validate their reconstruction, and correct reconstruction errors. The core of the method is the well-known concept of distributed source coding with decoder side information, fortified by a generalized-concatenation code construction enabling efficient embedding of all the information needed for reliable reconstruction. We first present the scheme for the case of substitution errors only between the reads and the reference, and then extend it to support reads with a single deletion and multiple substitutions. A central tool in this extension is a new distance metric that is shown analytically to improve alignment performance over existing distance metrics.

2022: Coding on Barrier Channels beyond Guaranteed Correction
Abstract: —This paper studies coding on channels with the barrier property: only errors to and from a special barrier state are possible. Our contributions include derivation of the channel capacity, efficient maximum-likelihood (ML) and list decoding algorithms, and finite-block-length analysis using random codes. Emerging non-volatile memory technologies may exhibit controlled unreliability as their representation power is increased, and thus may benefit from the high capacity and improved coding of barrier channels.

2022: Optimizing Write Fidelity of MRAMs by Alternating Water-Filling Algorithm
Abstract: Magnetic random-access memory (MRAM) is a promising memory technology due to its high density, non-volatility, and high endurance. However, achieving high memory fidelity incurs high write-energy costs, which should be reduced for large-scale deployment of MRAMs. In this paper, we formulate a biconvex optimization problem to optimize write fidelity given energy and latency constraints. The basic idea is to allocate non-uniform write pulses depending on the importance of each bit position. The fidelity measure we consider is mean squared error (MSE), for which we optimize write pulses via alternating convex search (ACS). We derive analytic solutions and propose an alternating water-filling algorithm by casting the MRAM’s write operation as communication over parallel channels. Hence, the proposed alternating water-filling algorithm is computationally more efficient than the original ACS while their solutions are identical. Since the formulated biconvex problem is non-convex, both the original ACS and the proposed algorithm do not guarantee global optimality. However, the MSEs obtained by the proposed algorithm are comparable to the MSEs by complicated global nonlinear programming solvers. Furthermore, we prove that our algorithm can reduce the MSE exponentially with the number of bits per word. For an 8-bit accessed word, the proposed algorithm reduces the MSE by a factor of 21. We also evaluate MNIST dataset classification supposing that the model parameters of deep neural networks are stored in MRAMs. The numerical results show that the optimized write pulses can achieve 40% write-energy reduction for the same classification accuracy.

2022: Generalized Longest Repeated Substring Min-Entropy Estimator
Abstract: The min-entropy is a widely used metric to quantify the randomness of generated random numbers, which measures the difficulty of guessing the most likely output. It is difficult to accurately estimate the min-entropy of a non-independent and identically distributed (non-IID) source. Hence, NIST Special Publication (SP) 800-90B adopts ten different min-entropy estimators and then conservatively selects the minimum value among ten min-entropy estimates. Among these estimators, the longest repeated substring (LRS) estimator estimates the collision entropy instead of the min-entropy by counting the number of repeated substrings. Since the collision entropy is an upper bound on the min-entropy, the LRS estimator inherently provides overestimated outputs. In this paper, we propose two techniques to estimate the min-entropy of a non-IID source accurately. The first technique resolves the overestimation problem by translating the collision entropy into the min-entropy. Next, we generalize the LRS estimator by adopting the general Rényi entropy instead of the collision entropy (i.e., Rényi entropy of order two). We show that adopting a higher order can reduce the variance of min-entropy estimates. By integrating these techniques, we propose a generalized LRS estimator that effectively resolves the overestimation problem and provides stable min-entropy estimates. Theoretical analysis and empirical results support that the proposed generalized LRS estimator improves the estimation accuracy significantly, which makes it an appealing alternative to the current-standard LRS estimator.

2022: Regression with an Ensemble of Noisy Base Functions
Abstract: Ensemble methods achieve state-of-the-art performance in many real-world regression problems while enjoying structural compatibility for modern decentralized computing architectures. However, the implementation of ensemble regression on distributed systems may compromise its cutting-edge performance due to computing and communication reliability issues. This paper introduces robust ensemble combining techniques designed to integrate multiple noisy predictions into a single reliable prediction. Experiments conducted with synthetic and real-world datasets in various noise regimes illustrate our robust methods' superiority over non-robust counterparts.

2022: Genomic Compression with Decoder Alignment under Single Deletion and Multiple Substitutions
Abstract: We address the problem of compressing genomic read data produced by modern shotgun sequencing technologies, where a reference genome, closely similar to the sequenced one, is available only at the decoder. This problem, addressed by distributed source coding techniques, requires an alignment and validation layer in the decoder. In this work, we extend a previous work, to allow a single deletion along with the previously addressed multiple substitutions. The results include a new distance for efficient alignment under deletion and substitutions, a derivation of the exact distribution of this distance on random sequences, as well as procedures to recover the read from multiple invocations of a substitutions-only decoder.

2022: A Unified Spatially Coupled Code Design: Threshold, Cycles, and Locality
Abstract: Spatially-Coupled (SC)-LDPC codes are known to have outstanding error-correction performance and low decoding latency. Whereas previous works on LDPC and SC-LDPC codes mostly take either an asymptotic or a finite-length design approach, in this paper we present a unified framework for jointly optimizing the codes' thresholds and cycle counts to address both regimes. The framework is based on efficient traversal and pruning of the code search space, building on the fact that the performance of a protograph-based SC-LDPC code depends on some characteristics of the code's partitioning matrix, which by itself is much smaller than the code's full parity-check matrix. We then propose an algorithm that traverses all nonequivalent partitioning matrices, and outputs a list of codes, each offering an attractive point on the trade-off between asymptotic and finite-length performance. We further extend the framework to designing SC-LDPC codes with sub-block locality, which is a recently introduced feature offering fast access to sub-blocks within the code block. Our simulations show that our framework results in SC-LDPC codes that outperform the state-of-the-art constructions, and that it offers the flexibility to choose low-SNR, high-SNR, or in-between SNR region as the primary design target.

2022: On the Decoding Performance of Spatially Coupled LDPC Codes With Sub-Block Access
Abstract: We study spatially coupled LDPC codes that allow access to sub-blocks much smaller than the full code block. Sub-block access is realized by a semi-global decoder that decodes a chosen target sub-block by only accessing the target, plus a prescribed number of helper sub-blocks adjacent in the code chain. This paper develops a theoretical methodology for analyzing the semi-global decoding performance of spatially coupled LDPC codes constructed from protographs. The main result shows that semi-global decoding thresholds can be derived from certain thresholds we define for the single-sub-block graph. These characterizing thresholds are also used for deriving lower bounds on the decoder’s performance over channels with variability across sub-blocks, which are motivated by applications in data storage.

2021: Optimizing Write Fidelity of MRAMs via Iterative Water-filling Algorithm
Abstract: Magnetic random-access memory (MRAM) is a promising memory technology due to its high density, non-volatility, and high endurance. However, achieving high memory fidelity incurs significant write-energy costs, which should be reduced for large-scale deployment of MRAMs. In this paper, we formulate a \emph{biconvex} optimization problem to optimize write fidelity given energy and latency constraints. The basic idea is to allocate non-uniform write pulses depending on the importance of each bit position. The fidelity measure we consider is mean squared error (MSE), for which we optimize write pulses via \emph{alternating convex search (ACS)}. By using Karush-Kuhn-Tucker (KKT) conditions, we derive analytic solutions and propose an \emph{iterative water-filling-type} algorithm by leveraging the analytic solutions. Hence, the proposed iterative water-filling algorithm is computationally more efficient than the original ACS while their solutions are identical. Although the original ACS and the proposed iterative water-filling algorithm do not guarantee global optimality, the MSEs obtained by the proposed algorithm are comparable to the MSEs by complicated global nonlinear programming solvers. Furthermore, we prove that the proposed algorithm can reduce the MSE exponentially with the number of bits per word. For an 8-bit accessed word, the proposed algorithm reduces the MSE by a factor of 21. We also evaluate the proposed algorithm for MNIST dataset classification supposing that the model parameters of deep neural networks are stored in MRAMs. The numerical results show that the optimized write pulses can achieve \SI{40}{\%} write energy reduction for a given classification accuracy.

2021: Coding on Dual-Parameter Barrier Channels beyond Worst-Case Correction
Abstract: This paper studies coding on channels with the barrier property: only errors to and from a special barrier state are possible. This model is motivated by storage media that have heterogeneous state structure, not admitting the usual multi-bit scaling of the representation states. Our contributions include derivation of the channel capacity, efficient maximum-likelihood and list decoding algorithms, and finite-block-length analysis using random codes. This work is the first that addresses a barrier channel with separate parameters for the transitions into and out of the barrier state. Earlier work addressed special-case single-parameter models, and focused primarily on the worst-case coding performance.

2021: Generalized LRS Estimator for Min-Entropy Estimation
Abstract: The min-entropy is a widely used metric to quantify the randomness of generated random numbers, which measures the difficulty of guessing the most likely output. It is difficult to accurately estimate the min-entropy of a non-independent and identically distributed (non-IID) source. Hence, NIST Special Publication (SP) 800-90B adopts ten different min-entropy estimators and then conservatively selects the minimum value among these ten min-entropy estimates. Among these estimators, the longest repeated substring (LRS) estimator estimates the collision entropy instead of the min-entropy by counting the number of repeated substrings. Since the collision entropy is an upper bound on the min-entropy, the LRS estimator inherently provides overestimated outputs. In this paper, we propose two techniques to estimate the min-entropy of a non-IID source accurately. The first technique resolves the overestimation problem by translating the collision entropy into the min-entropy. Next, we generalize the LRS estimator by adopting the general Rényi entropy instead of the collision entropy (i.e., Rényi entropy of order two). We show that adopting a higher order can reduce the variance of min-entropy estimates. By integrating these techniques, we propose a generalized LRS estimator that effectively resolves the overestimation problem and provides stable min-entropy estimates. Theoretical analysis and empirical results support that the proposed generalized LRS estimator improves the estimation accuracy significantly, which makes it an appealing alternative to the LRS estimator.

