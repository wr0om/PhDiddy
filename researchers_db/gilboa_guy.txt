Recent papers for Gilboa Guy:

2023: Minimizing Quotient Regularization Model
Abstract: Quotient regularization models (QRMs) are a class of powerful regularization techniques that have gained considerable attention in recent years, due to their ability to handle complex and highly nonlinear data sets. However, the nonconvex nature of QRM poses a significant challenge in finding its optimal solution. We are interested in scenarios where both the numerator and the denominator of QRM are absolutely one-homogeneous functions, which is widely applicable in the fields of signal processing and image processing. In this paper, we utilize a gradient flow to minimize such QRM in combination with a quadratic data fidelity term. Our scheme involves solving a convex problem iteratively.The convergence analysis is conducted on a modified scheme in a continuous formulation, showing the convergence to a stationary point. Numerical experiments demonstrate the effectiveness of the proposed algorithm in terms of accuracy, outperforming the state-of-the-art QRM solvers.

2023: EPiC: Ensemble of Partial Point Clouds for Robust Classification
Abstract: Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling.We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al. [24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity analysis. Our code is availabe at: https://github.com/yossilevii100/EPiC

2023: Enhancing Neural Training via a Correlated Dynamics Model
Abstract: As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we introduce Correlation Mode Decomposition (CMD). This algorithm clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes. Moreover, test set generalization is enhanced. We introduce an efficient CMD variant, designed to run concurrently with training. Our experiments indicate that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification. Our modeling can improve training efficiency and lower communication overhead, as shown by our preliminary experiments in the context of federated learning.

2023: Robustifying Point Cloud Networks by Refocusing
Abstract: The ability to cope with out-of-distribution (OOD) corruptions and adversarial attacks is crucial in real-world safety-demanding applications. In this study, we develop a general mechanism to increase neural network robustness based on focus analysis. Recent studies have revealed the phenomenon of \textit{Overfocusing}, which leads to a performance drop. When the network is primarily influenced by small input regions, it becomes less robust and prone to misclassify under noise and corruptions. However, quantifying overfocusing is still vague and lacks clear definitions. Here, we provide a mathematical definition of \textbf{focus}, \textbf{overfocusing} and \textbf{underfocusing}. The notions are general, but in this study, we specifically investigate the case of 3D point clouds. We observe that corrupted sets result in a biased focus distribution compared to the clean training set. We show that as focus distribution deviates from the one learned in the training phase - classification performance deteriorates. We thus propose a parameter-free \textbf{refocusing} algorithm that aims to unify all corruptions under the same distribution. We validate our findings on a 3D zero-shot classification task, achieving SOTA in robust 3D classification on ModelNet-C dataset, and in adversarial defense against Shape-Invariant attack. Code is available in: https://github.com/yossilevii100/refocusing.

2023: Latent Modes of Nonlinear Flows
Abstract: Extracting the latent underlying structures of complex nonlinear local and nonlocal flows is essential for their analysis and modeling. In this Element the authors attempt to provide a consistent framework through Koopman theory and its related popular discrete approximation - dynamic mode decomposition (DMD). They investigate the conditions to perform appropriate linearization, dimensionality reduction and representation of flows in a highly general setting. The essential elements of this framework are Koopman eigenfunctions (KEFs) for which existence conditions are formulated. This is done by viewing the dynamic as a curve in state-space. These conditions lay the foundations for system reconstruction, global controllability, and observability for nonlinear dynamics. They examine the limitations of DMD through the analysis of Koopman theory and propose a new mode decomposition technique based on the typical time profile of the dynamics.

2023: Graph Laplacian for Semi-Supervised Learning
Abstract: Semi-supervised learning is highly useful in common scenarios where labeled data is scarce but unlabeled data is abundant. The graph (or nonlocal) Laplacian is a fundamental smoothing operator for solving various learning tasks. For unsupervised clustering, a spectral embedding is often used, based on graph-Laplacian eigenvectors. For semi-supervised problems, the common approach is to solve a constrained optimization problem, regularized by a Dirichlet energy, based on the graph-Laplacian. However, as supervision decreases, Dirichlet optimization becomes suboptimal. We therefore would like to obtain a smooth transition between unsupervised clustering and low-supervised graph-based classification. In this paper, we propose a new type of graph-Laplacian which is adapted for Semi-Supervised Learning (SSL) problems. It is based on both density and contrastive measures and allows the encoding of the labeled data directly in the operator. Thus, we can perform successfully semi-supervised learning using spectral clustering. The benefits of our approach are illustrated for several SSL problems.

2023: Additive Class Distinction Maps using Branched-GANs
Abstract: We present a new model, training procedure and architecture to create precise maps of distinction between two classes of images. The objective is to comprehend, in pixel-wise resolution, the unique characteristics of a class. These maps can facilitate self-supervised segmentation and objectdetection in addition to new capabilities in explainable AI (XAI). Our proposed architecture is based on image decomposition, where the output is the sum of multiple generative networks (branched-GANs). The distinction between classes is isolated in a dedicated branch. This approach allows clear, precise and interpretable visualization of the unique characteristics of each class. We show how our generic method can be used in several modalities for various tasks, such as MRI brain tumor extraction, isolating cars in aerial photography and obtaining feminine and masculine face features. This is a preliminary report of our initial findings and results.

2022: Spectral Total-variation Processing of Shapes—Theory and Applications
Abstract: We present a comprehensive analysis of total variation (TV) on non-Euclidean domains and its eigenfunctions. We specifically address parameterized surfaces, a natural representation of the shapes used in 3D graphics. Our work sheds new light on the celebrated Beltrami and Anisotropic TV flows and explains experimental findings from recent years on shape spectral TV [Fumero et al. 2020] and adaptive anisotropic spectral TV [Biton and Gilboa 2022]. A new notion of convexity on surfaces is derived by characterizing structures that are stable throughout the TV flow, performed on surfaces. We establish and numerically demonstrate quantitative relationships between TV, area, eigenvalue, and eigenfunctions of the TV operator on surfaces. Moreover, we expand the shape spectral TV toolkit to include zero-homogeneous flows, leading to efficient and versatile shape processing methods. These methods are exemplified through applications in smoothing, enhancement, and exaggeration filters. We introduce a novel method that, for the first time, addresses the shape deformation task using TV. This deformation technique is characterized by the concentration of deformation along geometrical bottlenecks, shown to coincide with the discontinuities of eigenfunctions. Overall, our findings elucidate recent experimental observations in spectral TV, provide a diverse framework for shape filtering, and present the first TV-based approach to shape deformation.

2022: How to Guide Adaptive Depth Sampling?
Abstract: Recent advances in depth sensing technologies allow fast electronic maneuvering of the laser beam, as opposed to fixed mechanical rotations. This will enable future sensors, in principle, to vary in real-time the sampling pattern. We examine here the abstract problem of whether adapting the sampling pattern for a given frame can reduce the reconstruction error or allow a sparser pattern. We propose a constructive generic method to guide adaptive depth sampling algorithms. Given a sampling budget B, a depth predictor P and a desired quality measure M, we propose an Importance Map that highlights important sampling locations. This map is defined for a given frame as the per-pixel expected value of M produced by the predictor P, given a pattern of B random samples. This map can be well estimated in a training phase. We show that a neural network can learn to produce a highly faithful Importance Map, given an RGB image. We then suggest an algorithm to produce a sampling pattern for the scene, which is denser in regions that are harder to reconstruct. The sampling strategy of our modular framework can be adjusted according to hardware limitations, type of depth predictor, and any custom reconstruction error measure that should be minimized. We validate through simulations that our approach outperforms grid and random sampling patterns as well as recent state-of-the-art adaptive algorithms.

2022: Adaptive Anisotropic Total Variation: Analysis and Experimental Findings of Nonlinear Spectral Properties
Abstract: None

2022: The Underlying Correlated Dynamics in Neural Training
Abstract: ,

2022: Analysis of Branch Specialization and its Application in Image Decomposition
Abstract: Branched neural networks have been used extensively for a variety of tasks. Branches are sub-parts of the model that perform independent processing followed by aggregation. It is known that this setting induces a phenomenon called Branch Specialization, where different branches become experts in different sub-tasks. Such observations were qualitative by nature. In this work, we present a methodological analysis of Branch Specialization. We explain the role of gradient descent in this phenomenon. We show that branched generative networks naturally decompose animal images to meaningful channels of fur, whiskers and spots and face images to channels such as different illumination components and face parts.

2022: BASiS: Batch Aligned Spectral Embedding Space
Abstract: Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spectral graph theory has been shown to provide powerful algorithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network building blocks with spectral graph characteristics. For instance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the graph's eigensapce. A severe problem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable alignment mechanism that can work both with batch changes and with graph-metric changes. We show that our learnt spectral embedding is better in terms of NMI, ACC, Grassman distnace, orthogonality and classification accuracy, compared to SOTA. In addition, the learning is more stable.

2021: A Pseudo-Inverse for Nonlinear Operators
Abstract: The Moore-Penrose inverse is widely used in physics, statistics and various ﬁelds of engineering. Among other characteristics, it captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we deﬁne and characterize the fundamental properties of a pseudo-inverse for nonlinear operators. The concept is deﬁned broadly. First for general sets, and then a reﬁnement for normed spaces. Our pseudo-inverse for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a pseudo-inverse and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the pseudo-inverse of some well-known, non-invertible, nonlinear operators, such as hard-or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding and to regularized loss minimization.

2021: Revealing stable and unstable modes of denoisers through nonlinear eigenvalue analysis
Abstract: None

2021: Nonlinear Spectral Processing of Shapes via Zero-Homogeneous Flows
Abstract: None

2021: Examining the Limitations of Dynamic Mode Decomposition through Koopman Theory Analysis
Abstract: This work binds the existence of Koopman Eigenfunction (KEF), the geometric of the dynamics, and the validity of Dynamic Mode Decomposition (DMD) to one coherent theory. Viewing the dynamic as a curve in the state-space allows us to formulate an existence condition of KEFs and their multiplicities. These conditions lay the foundations for system reconstruction, global controllability, and observability for nonlinear dynamics. DMD can be interpreted as a finite dimension approximation of Koopman Mode Decomposition (KMD). However, this method is limited to the case when KEFs are linear combinations of the observations. We examine the limitations of DMD through the analysis of Koopman theory. We propose a new mode decomposition technique based on the typical time profile of the dynamics. An overcomplete dictionary of decay profiles is used to sparsely represent the dynamic. This analysis is also valid in the full continuous setting of Koopman theory, which is based on variational calculus. We demonstrate applications of this analysis, such as finding KEFs and their multiplicities, calculating KMD, dynamics reconstruction, global linearization, and controllability. ?This work was supported by the European Union’s Horizon 2020 research and innovation programme under the Marie Sk lodowska-Curie grant agreement No. 777826 (NoMADS). GG acknowledges support by the Israel Science Foundation (Grant No. 534/19) and by the Ollendorff Minerva Center. ?Corresponding author. E-mail addresses: idoc@campus.technion.ac.il,ido.coh@gmail.com Preprint submitted to Journal of LTEX Templates August 9, 2021 ar X iv :2 10 7. 07 45 6v 2 [ m at h. D S] 5 A ug 2 02 1

2021: Generalized Inversion of Nonlinear Operators
Abstract: None

2021: Modes of Homogeneous Gradient Flows
Abstract: Finding latent structures in data is drawing increasing attention in diverse fields such as image and signal processing, fluid dynamics, and machine learning. In this work we examine the problem of...

2021: Latent Modes of Nonlinear Flows -- a Koopman Theory Analysis
Abstract: Extracting the latent underlying structures of complex nonlinear local and nonlocal flows is essential for their analysis and modeling. In this work we attempt to provide a consistent framework through Koopman theory and its related popular discrete approximation – dynamic mode decomposition (DMD). We investigate the conditions to perform appropriate linearization, dimensionality reduction and representation of flows in a highly general setting. The essential elements of this framework are Koopman Eigenfunction (KEF), for which existence conditions are formulated. This is done by viewing the dynamic as a curve in state-space. These conditions lay the foundations for system reconstruction, global controllability, and observability for nonlinear dynamics. We examine the limitations of DMD through the analysis of Koopman theory and propose a new mode decomposition technique based on the typical time profile of the dynamics. An overcomplete dictionary of decay profiles is used to sparsely approximate the flow. This analysis is also valid in the full continuous setting of Koopman theory, which is based on variational calculus. We demonstrate applications of this analysis, such as finding KEFs and their multiplicities, dynamics reconstruction and global linearization.

