Recent papers for Tal Ido:

2025: Constant Weight Polar Codes through Periodic Markov Processes
Abstract: Constant weight codes can arise from an input process sampled from a periodic Markov chain. A previous result showed that, in general, polarization does not occur for input-output processes with an underlying periodic Markov chain. In this work, we show that if we fix the initial state of an underlying periodic Markov chain, polarization does occur. Fixing the initial state is aligned with ensuring a constant weight code.

2025: An Analytical Study of the Min-Sum Approximation for Polar Codes
Abstract: The min-sum approximation is widely used in the decoding of polar codes. Although it is a numerical approximation, hardly any penalties are incurred in practice. We give a theoretical justification for this. We consider the common case of a binary-input, memoryless, and symmetric channel, decoded using successive cancellation and the min-sum approximation. Under mild assumptions, we show the following. For the finite length case, we show how to exactly calculate the error probabilities of all synthetic (bit) channels in time $O(N^{1.585})$, where $N$ is the codeword length. This implies a code construction algorithm with the above complexity. For the asymptotic case, we develop two rate thresholds, denoted $R_{\mathrm{L}} = R_{\mathrm{L}}(\lambda)$ and $R_{\mathrm{U}} =R_{\mathrm{U}}(\lambda)$, where $\lambda(\cdot)$ is the labeler of the channel outputs (essentially, a quantizer). For any $0<\beta<\frac{1}{2}$ and any code rate $R<R_{\mathrm{L}}$, there exists a family of polar codes with growing lengths such that their rates are at least $R$ and their error probabilities are at most $2^{-N^\beta}$. That is, strong polarization continues to hold under the min-sum approximation. Conversely, for code rates exceeding $R_{\mathrm{U}}$, the error probability approaches $1$ as the code-length increases, irrespective of which bits are frozen. We show that $0<R_{\mathrm{L}} \leq R_{\mathrm{U}} \leq C$, where $C$ is the channel capacity. The last inequality is often strict, in which case the ramification of using the min-sum approximation is that we can no longer achieve capacity.

2024: Strong Polarization for Shortened and Punctured Polar Codes
Abstract: Polar codes were originally specified for codelengths that are powers of two. In many applications, it is desired to have a code that is not restricted to such lengths. Two common strategies of modifying the length of a code are shortening and puncturing. Simple and explicit schemes for shortening and puncturing were introduced by Wang and Liu, and by Niu, Chen, and Lin, respectively. In this paper, we prove that both schemes yield polar codes that are capacity achieving. Moreover, the probability of error for both the shortened and the punctured polar codes decreases to zero at the same exponential rate as seminal polar codes. These claims hold for all codelengths large enough.

2023: Stronger Polarization for the Deletion Channel
Abstract: In this paper we show a polar coding scheme for the deletion channel with a probability of error that decays roughly like ${2^{ - \sqrt \lambda }}$ , where Λ is the length of the codeword. That is, the same decay rate as that of seminal polar codes for memoryless channels. This is stronger than prior art in which the square root is replaced by a cube root. Our coding scheme is similar yet distinct from prior art. The main differences are: 1) Guard-bands are placed in almost all polarization levels; 2) Trellis decoding is applied to the whole received word, and not to segments of it. As before, the scheme is capacity-achieving. The price we pay for this improvement is a higher decoding complexity, which is nonetheless still polynomial, O(Λ4).

2023: Dimensions of Channel Coding: From Theory to Algorithms to Applications
Abstract: None

2022: Polar Codes for the Deletion Channel: Weak and Strong Polarization
Abstract: This paper presents the first proof of polarization for the deletion channel with a constant deletion rate and a regular hidden-Markov input distribution. A key part of this work involves representing the deletion channel using a trellis and describing the plus and minus polar-decoding operations on that trellis. In particular, the plus and minus operations can be seen as combining adjacent trellis stages to yield a new trellis with half as many stages. Using this viewpoint, we prove a weak polarization theorem for standard polar codes on the deletion channel. To achieve strong polarization, we modify this scheme by adding guard bands of repeated zeros between various parts of the codeword. This gives a scheme whose rate approaches the mutual information and whose probability of error decays exponentially in the cube-root of the block length. We conclude by showing that this scheme can achieve capacity on the deletion channel by proving that the capacity of the deletion channel can be achieved by a sequence of regular hidden-Markov input distributions.

2021: Polar Codes for Channels with Insertions, Deletions, and Substitutions
Abstract: This paper presents a coding scheme for an insertion deletion substitution channel. We extend a previous scheme for the deletion channel where polar codes are modified by adding “guard bands” between segments. In the new scheme, each guard band is comprised of a middle segment of ‘1’ symbols, and left and right segments of ‘0’ symbols. Our coding scheme allows for a regular hidden-Markov input distribution, and achieves the information rate between the input and corresponding output of such a distribution. Thus, we prove that our scheme can be used to efficiently achieve the capacity of the channel. The probability of error of our scheme decays exponentially in the cube-root of the block length.

2020: An Upgrading Algorithm With Optimal Power Law
Abstract: Consider a channel <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> along with a given input distribution <inline-formula> <tex-math notation="LaTeX">$P_{X}$ </tex-math></inline-formula>. In certain settings, such as in the construction of polar codes, the output alphabet of <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> is ‘too large’, and hence we replace <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> by a channel <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> having a smaller output alphabet. We say that <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> is upgraded with respect to <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> if <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> is obtained from <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> by processing its output. In this case, the mutual information <inline-formula> <tex-math notation="LaTeX">$I(P_{X},W)$ </tex-math></inline-formula> between the input and output of <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> is upper-bounded by the mutual information <inline-formula> <tex-math notation="LaTeX">$I(P_{X},Q)$ </tex-math></inline-formula> between the input and output of <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula>. In this paper, we present an algorithm that produces an upgraded channel <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> from <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula>, as a function of <inline-formula> <tex-math notation="LaTeX">$P_{X}$ </tex-math></inline-formula> and the required output alphabet size of <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula>, denoted <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula>. We show that the difference in mutual informations is ‘small’. Namely, it is <inline-formula> <tex-math notation="LaTeX">$O(L^{-2/(| \mathcal {X}|-1)})$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation="LaTeX">$| \mathcal {X}|$ </tex-math></inline-formula> is the size of the input alphabet. This power law of <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula> is optimal. We complement our analysis with numerical experiments which show that the developed algorithm improves upon the existing state-of-the-art algorithms also in non-asymptotic setups.

2020: List Decoding of Universal Polar Codes
Abstract: A list decoding scheme for universal polar codes is presented. Our scheme applies to the universal polar codes first introduced by Şaşoğlu and Wang, and generalized to processes with memory by the authors. These codes are based on the concatenation of different polar transforms: a sequence of "slow" transforms and Arıkan’s original "fast" transform. List decoding of polar codes has been previously presented in the context of the fast transform. However, the slow transform is markedly different and requires new techniques and data structures. We show that list decoding is possible with space complexity $O({\mathcal{L}}\cdot N)$ and time complexity $O({\mathcal{L}}\cdot N\log N)$, where N is the overall blocklength and ${\mathcal{L}}$ is the list size.

2019: Greedy–Merge Degrading has Optimal Power-Law
Abstract: Consider a channel with a given input alphabet size and input distribution. Our aim is to degrade or upgrade it to a channel with at most <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula> output letters. Channel <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> is degraded with respect to a channel <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> if <inline-formula> <tex-math notation="LaTeX">$Q$ </tex-math></inline-formula> can be obtained from <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula> by processing the output of <inline-formula> <tex-math notation="LaTeX">$W$ </tex-math></inline-formula>. Upgrading is the inverse relation. This paper contains four main results. The first result, from which the paper title is derived, deals with the so called “greedy–merge” algorithm. We derive an upper bound on the reduction in mutual information between input and output, as a function of <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula>. This upper bound is within a constant factor of an algorithm-independent lower bound. Thus, we establish that greedy–merge is optimal in the power-law sense (i.e. the power of <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula>). The other main results deal with upgrading. The second result shows that a certain sequence of channels, which was previously shown to be “hard” for degrading, displays the same hardness in the context of upgrading. That is, suppose we are given such a channel and a corresponding input distribution. If we upgrade (degrade) to a new channel with <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula> output letters, we incur an increase (decrease) in mutual information between input and output. We show that a previously derived bound on the decrease in mutual information for the degrading case is also a lower bound on the increase for the upgrading case. The third result is an efficient algorithm for optimal upgrading, in the binary-input case. That is, we are given a channel and an input distribution. We must find an upgraded channel with <inline-formula> <tex-math notation="LaTeX">$L$ </tex-math></inline-formula> output letters, for which the increase in mutual information is minimal. We give a simple characterization of such a channel, which implies an efficient algorithm. The fourth result is an analog of the first result for the upgrading case when the input is binary. That is, we first present a sub-optimal algorithm for the setting considered in the third result. The main advantage of the sub-optimal algorithm is that it is amenable to analysis. We carry out the analysis and show that the increase incurred in mutual information is within a constant factor of the lower bound derived in the second result.

2019: A Lower Bound on the Probability of Error of Polar Codes over BMS Channels
Abstract: Polar codes are a family of capacity-achieving codes that have explicit and low-complexity construction, encoding, and decoding algorithms. Decoding of polar codes is based on the successive-cancellation decoder, which decodes in a bit-wise manner. A decoding error occurs when at least one bit is erroneously decoded. The various codeword bits are correlated, yet performance analysis of polar codes ignores this dependence: the upper bound is based on the union bound, and the lower bound is based on the worst-performing bit. Improvement of the lower bound is afforded by considering error probabilities of two bits simultaneously. These are difficult to compute explicitly due to the large alphabet size inherent to polar codes. In this paper, we propose a method to lower-bound the error probabilities of bit pairs. We develop several transformations on pairs of synthetic channels that make the resultant synthetic channels amenable to alphabet reduction. Our method yields lower bounds that significantly improve upon currently known lower bounds for polar codes under successive-cancellation decoding.

2019: Polar Coding for Processes With Memory
Abstract: We study polar coding for stochastic processes with memory. For example, a process may be defined by the joint distribution of the input and output of a channel. The memory may be present in the channel, the input, or both. We show that the <inline-formula> <tex-math notation="LaTeX">${\psi }$ </tex-math></inline-formula>-mixing processes polarize under the standard Arıkan transform, under a mild condition. We further show that the rate of polarization of the <italic>low-entropy</italic> synthetic channels is roughly <inline-formula> <tex-math notation="LaTeX">${O}({2}^{-\sqrt {N}})$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation="LaTeX">${N}$ </tex-math></inline-formula> is the blocklength. That is, essentially, the same rate as in the memoryless case.

2019: Fast Polarization for Processes With Memory
Abstract: Fast polarization is crucial for the performance guarantees of polar codes. In the memoryless setting, the rate of polarization is known to be exponential in the square root of the block length. A complete characterization of the rate of polarization for models with memory has been missing. Namely, previous works have not addressed fast polarization of the high entropy set under memory. We consider polar codes for processes with memory that are characterized by an underlying ergodic finite-state Markov chain. We show that the rate of polarization for these processes is the same as in the memoryless setting, both for the high and for the low entropy sets.

2019: Polar Codes for the Deletion Channel: Weak and Strong Polarization
Abstract: This paper presents the first proof of polarization for the deletion channel with a constant deletion rate and a regular hidden-Markov input distribution. A key part of this work involves representing the deletion channel using a trellis and describing the plus and minus polar-decoding operations on this trellis. In particular, the plus and minus operations can be seen as combining adjacent trellis stages to yield a new trellis with half as many stages. Using this viewpoint, we prove a weak polarization theorem for standard polar codes on the deletion channel. To achieve strong polarization, we modify this scheme by adding guard bands of repeated zeros between various parts of the codeword. Using this approach, we obtain a scheme whose rate approaches the mutual information and whose probability of error decays exponentially in the cube-root of the block length.

2019: Best Readings in Polar Coding
Abstract: None

2018: List Decoding of Lee Metric Codes Research
Abstract: 1 Abbreviations and Notations 2

2018: A Simple Proof of Fast Polarization
Abstract: Fast polarization is a key property of polar codes. It was proved for the binary polarizing 2× 2 kernel by Arıkan and Telatar. The proof was later adapted to the general case by Şaşoğlu. We give a simplified proof.

2018: Universal Polarization for Processes with Memory
Abstract: A transform that is universally polarizing over a set of channels with memory is presented. Memory may be present in both the channel and its input. Both the encoder and the decoder are aware of the input distribution, which is fixed. Only the decoder is aware of the actual channel being used. The transform is used to design a universal code for this scenario. The code is to have vanishing error probability when used over any channel in the set, and achieve the infimal information rate over the set. Universal polarization is established under two key properties: memory in the form of an underlying hidden Markov state sequence that is aperiodic and irreducible and a new property: forgetfulness.

2017: Fast Polarization for Processes with Memory
Abstract: Fast polarization is crucial for the performance guarantees of polar codes. In the memoryless setting, the rate of polarization is known to be exponential in the square root of the block length. A complete characterization of the rate of polarization for models with memory has been missing. We consider polar codes for processes with memory that are characterized by an underlying aperiodic and irreducible finite state Markov chain. We show that the rate of polarization for these processes is the same as in the memoryless setting, both to the high and to the low-entropy sets. Thus, polar codes achieve the Markov capacity in many information-theoretic applications.

2017: Channel Upgradation for Non-Binary Input Alphabets and MACs
Abstract: Consider a single-user or multiple-access channel with a large output alphabet. A method to approximate the channel by an upgraded version having a smaller output alphabet is presented and analyzed. The original channel is not necessarily symmetric and does not necessarily have a binary input alphabet. Also, the input distribution is not necessarily uniform. The approximation method is instrumental when constructing capacity achieving polar codes for an asymmetric channel with a non-binary input alphabet. Other settings in which the method is instrumental are the wiretap setting as well as the lossy source coding setting.

