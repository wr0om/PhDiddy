2024: Robust Radiotherapy Planning with Spatially Based Uncertainty Sets
Abstract: Radiotherapy treatment planning is a challenging large-scale optimization problem plagued by uncertainty. Following the robust optimization methodology, we propose a novel, spatially based uncertainty set for robust modeling of radiotherapy planning, producing solutions that are immune to unexpected changes in biological conditions. Our proposed uncertainty set realistically captures biological radiosensitivity patterns that are observed using recent advances in imaging, while its parameters can be personalized for individual patients. We exploit the structure of this set to devise a compact reformulation of the robust model. We develop a row-generation scheme to solve real, large-scale instances of the robust model. This method is then extended to a relaxation-based scheme for enforcing challenging, yet clinically important, dose-volume cardinality constraints. The computational performance of our algorithms, as well as the quality and robustness of the computed treatment plans, are demonstrated on simulated and real imaging data. Based on accepted performance measures, such as minimal target dose and homogeneity, these examples demonstrate that the spatially robust model achieves almost the same performance as the nominal model in the nominal scenario, and otherwise, the spatial model outperforms both the nominal and the box-uncertainty models.

2024: Code and Data Repository for First-order algorithms for robust optimization problems via convex-concave saddle-point Lagrangian reformulation
Abstract: The software and data in this repository are a snapshot of the software and data that were used in the research reported on in the paper 'First-order algorithms for robust optimization problems via convex-concave saddle-point Lagrangian reformulation' (https://doi.org/10.1287/ijoc.2022.0200) by K. Postek and S. Shtern.

2023: A Data-Driven Approach to Multistage Stochastic Linear Optimization
Abstract: We propose a new data-driven approach for addressing multi-stage stochastic linear optimization problems with unknown distributions. The approach consists of solving a robust optimization problem that is constructed from sample paths of the underlying stochastic process. As more sample paths are obtained, we prove that the optimal cost of the robust problem converges to that of the underlying stochastic problem. To the best of our knowledge, this is the first data-driven approach for multi-stage stochastic linear optimization which is asymptotically optimal when uncertainty is arbitrarily correlated across time. Finally, we develop approximation algorithms for the proposed approach by extending techniques from the robust optimization literature, and demonstrate their practical value through numerical experiments on stylized data-driven inventory management problems.

2023: Nested Alternating Minimization with FISTA for Non-convex and Non-smooth Optimization Problems
Abstract: None

2022: Convergent Nested Alternating Minimization Algorithms for Nonconvex Optimization Problems
Abstract: We introduce a new algorithmic framework for solving nonconvex optimization problems, that is called nested alternating minimization, which aims at combining the classical alternating minimization technique with inner iterations of any optimization method. We provide a global convergence analysis of the new algorithmic framework to critical points of the problem at hand, which to the best of our knowledge, is the first of this kind for nested methods in the nonconvex setting. Central to our global convergence analysis is a new extension of classical proof techniques in the nonconvex setting that allows for errors in the conditions. The power of our framework is illustrated with some numerical experiments that show the superiority of this algorithmic framework over existing methods.

2022: A conditional gradient homotopy method with applications to Semidefinite Programming
Abstract: We propose a new homotopy-based conditional gradient method for solving convex optimization problems with a large number of simple conic constraints. Instances of this template naturally appear in semidefinite programming problems arising as convex relaxations of combinatorial optimization problems. Our method is a double-loop algorithm in which the conic constraint is treated via a self-concordant barrier, and the inner loop employs a conditional gradient algorithm to approximate the analytic central path, while the outer loop updates the accuracy imposed on the temporal solution and the homotopy parameter. Our theoretical iteration complexity is competitive when confronted to state-of-the-art SDP solvers, with the decisive advantage of cheap projection-free subroutines. Preliminary numerical experiments are provided for illustrating the practical performance of the method.

2022: Methodology and first-order algorithms for solving nonsmooth and non-strongly convex bilevel optimization problems
Abstract: None

2021: An adaptive robust optimization model for parallel machine scheduling
Abstract: None

2021: First-Order Algorithms for Robust Optimization Problems via Convex-Concave Saddle-Point Lagrangian Reformulation
Abstract: Robust optimization (RO) is one of the key paradigms for solving optimization problems affected by uncertainty. Two principal approaches for RO, the robust counterpart method and the adversarial approach, potentially lead to excessively large optimization problems. For that reason, first-order approaches, based on online convex optimization, have been proposed as alternatives for the case of large-scale problems. However, existing first-order methods are either stochastic in nature or involve a binary search for the optimal value. We show that this problem can also be solved with deterministic first-order algorithms based on a saddle-point Lagrangian reformulation that avoid both of these issues. Our approach recovers the other approaches’ [Formula: see text] convergence rate in the general case and offers an improved [Formula: see text] rate for problems with constraints that are affine both in the decision and in the uncertainty. Experiment involving robust quadratic optimization demonstrates the numerical benefits of our approach. History: Accepted by Antonio Frangioni, Area Editor for Design & Analysis of Algorithms–Continuous. Funding: This work was supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant VI.Veni.191E.035] and the Israel Science Foundation [Grant 1460/19]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2022.0200 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2022.0200 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .

2021: First-Order Methods for Convex Optimization
Abstract: None

2021: Technical Note - Two-Stage Sample Robust Optimization
Abstract: In “Two-Stage Sample Robust Optimization,” Bertsimas, Shtern, and Sturt investigate a simple approximation scheme, based on overlapping linear decision rules, for solving data-driven two-stage distributionally robust optimization problems with the type-infinity Wasserstein ambiguity set. Their main result establishes that this approximation scheme is asymptotically optimal for two-stage stochastic linear optimization problems; that is, under mild assumptions, the optimal cost and optimal first-stage decisions obtained by approximating the robust optimization problem converge to those of the underlying stochastic problem as the number of data points grows to infinity. These guarantees notably apply to two-stage stochastic problems that do not have relatively complete recourse, which arise frequently in applications. In this context, the authors show through numerical experiments that the approximation scheme is practically tractable and produces decisions that significantly outperform those obtained from state-of-the-art data-driven alternatives.

2020: Alternating Minimization Based First-Order Method for the Wireless Sensor Network Localization Problem
Abstract: We propose an algorithm for the Wireless Sensor Network localization problem, which is based on the well-known algorithmic framework of Alternating Minimization. We start with a non-smooth and non-convex minimization, and transform it into an equivalent smooth and non-convex problem, which stands at the heart of our study. This paves the way to a new method which is globally convergent: not only does the sequence of objective function values converge, but the sequence of the location estimates also converges to a unique location that is a critical point of the corresponding (original) objective function. The proposed algorithm has a range of fully distributed to fully centralized implementations, which all have the property of global convergence. The algorithm is tested over several network configurations, and it is shown to produce more accurate solutions within a shorter time relative to existing methods.

2020: Generalized self-concordant analysis of Frank–Wolfe algorithms
Abstract: None

2020: Self-concordant analysis of Frank-Wolfe algorithms
Abstract: Projection-free optimization via different variants of the Frank-Wolfe (FW), a.k.a. Conditional Gradient method has become one of the cornerstones in optimization for machine learning since in many cases the linear minimization oracle is much cheaper to implement than projections and some sparsity needs to be preserved. In a number of applications, e.g. Poisson inverse problems or quantum state tomography, the loss is given by a self-concordant (SC) function having unbounded curvature, implying absence of theoretical guarantees for the existing FW methods. We use the theory of SC functions to provide a new adaptive step size for FW methods and prove global convergence rate O(1/k) after k iterations. If the problem admits a stronger local linear minimization oracle, we construct a novel FW method with linear convergence rate for SC functions.

2019: Two-stage sample robust optimization
Abstract: We investigate a data-driven approach to two-stage stochastic linear optimization in which an uncertainty set is constructed around each data point. We propose an approximation algorithm for these sample robust optimization problems by optimizing a separate linear decision rule for each uncertainty set. We show that the proposed algorithm combines the asymptotic optimality and scalability of the sample average approximation while simultaneously offering improved out-of-sample performance guarantees. The practical value of our method is demonstrated in network inventory management and hospital scheduling.

2018: A Scalable Algorithm for Two-Stage Adaptive Linear Optimization
Abstract: The column-and-constraint generation (CCG) method was introduced by \citet{Zeng2013} for solving two-stage adaptive optimization. We found that the CCG method is quite scalable, but sometimes, and in some applications often, produces infeasible first-stage solutions, even though the problem is feasible. In this research, we extend the CCG method in a way that (a) maintains scalability and (b) always produces feasible first-stage decisions if they exist. We compare our method to several recently proposed methods and find that it reaches high accuracies faster and solves significantly larger problems.

2018: Multitarget Tracking via Mixed Integer Optimization
Abstract: Given a set of target detections over several time periods, this paper addresses the multitarget tracking (MTT) problem of optimally assigning detections to targets and estimating the trajectory of the targets over time. MTT has been studied in the literature via predominantly probabilistic methods. In contrast, we propose the use of mixed integer optimization (MIO) along with relaxations and local-search-based heuristic algorithms that are: scalable, as they provide near optimal solutions for six targets and ten time periods in milliseconds to seconds; general, as they make no probabilistic assumptions on the detection process; robust, as they can accommodate missed and false detections of the targets; and easily implementable, as they use at most two tuning parameters. We evaluate the performance of the new methods using a novel metric for the complexity of an instance, and find that they provide high quality solutions both reliably and quickly for a large range of scenarios, resulting in a promising approach to the area of MTT.

2017: A First Order Method for Solving Convex Bilevel Optimization Problems
Abstract: In this paper we study convex bilevel optimization problems for which the inner level consists of minimization of the sum of smooth and nonsmooth functions. The outer level aims at minimizing a smooth and strongly convex function over the optimal solutions set of the inner problem. We analyze a first order method which is based on an existing fixed-point algorithm. Global sublinear rate of convergence of the method is established in terms of the inner objective function values.

2016: Computational Methods for Solving Nonconvex Block-Separable Constrained Quadratic Problems
Abstract: Nonconvex quadratically constrained quadratic programs (QCQPs) with block-separable convex constraints are generally NP-hard. These kinds of problems appear in many applications such as estimation and control, complex unimodular programming, and MAX-CUT type problems. Semidefinite relaxation is the best known upper bound approximation for QCQP with block-separable constraints. We suggest the block optimal descent (BOD) algorithm to obtain a lower approximation. We show that this algorithm utilizes block hidden convexity to apply block alternating minimization and has a sublinear rate of convergence. An improved approximation is obtained by using a novel approach, Lagrange guided descent (LGD), which finds a “good” initial point based on the semidefinite programming (SDP) relaxation solution. A quantitative study shows the LGD has superior performance over BOD.

2015: Linearly convergent away-step conditional gradient for non-strongly convex functions
Abstract: None

