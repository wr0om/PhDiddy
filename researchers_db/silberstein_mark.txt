Recent papers for Silberstein Mark:

2024: In-Network Address Caching for Virtual Networks
Abstract: None

2023: Fuzzing LibraryOSes for Iago vulnerabilities
Abstract: We present a new fuzzing approach for Iago vulnerabilities in Library OSes for SGX enclaves. Based on the filesystem model, it allows efficiently combining valid and malicious values to reach deeper paths in LibraryOS to identify more potential security vulnerabilities.

2023: Scaling by Learning: Accelerating Open vSwitch Data Path With Neural Networks
Abstract: Open vSwitch (OVS) is a widely used open-source virtual switch implementation. In this work, we seek to scale up OVS to support hundreds of thousands of OpenFlow rules by accelerating the core component of its data-path - the packet classification mechanism. To do so we use NuevoMatch, a recent algorithm that uses neural network inference to match packets, and promises significant scalability and performance benefits. We overcome the primary algorithmic challenge of the slow training rate in the vanilla NuevoMatch, speeding it up by over three orders of magnitude. This improvement enables two design options to integrate NuevoMatch with OVS: (1) as an extra caching layer in front of OVS‚Äôs megaflow cache, and (2) using it to completely replace OVS‚Äôs data-path while performing classification directly on OpenFlow rules, and obviating control-path upcalls. Comprehensive evaluation on real-world packet traces and ClassBench rules demonstrates geometric mean speedups of <inline-formula> <tex-math notation="LaTeX">$1.9\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation="LaTeX">$12.3\times $ </tex-math></inline-formula> for the first and second designs, respectively, for 500K rules, with the latter also supporting up to 60K OpenFlow rule updates/second, by far exceeding the original OVS.

2023: Hide and Seek with Spectres: Efficient discovery of speculative information leaks with random testing
Abstract: Attacks like Spectre abuse speculative execution, one of the key performance optimizations of modern CPUs. Recently, several testing tools have emerged to automatically detect speculative leaks in commercial (black-box) CPUs. However, the testing process is still slow, which has hindered in-depth testing campaigns, and so far prevented the discovery of new classes of leakage.In this paper, we identify the root causes of the performance limitations in existing approaches, and propose techniques to overcome these limitations. With these techniques, we improve the testing speed over the state-of-the-art by up to two orders of magnitude.These improvements enable us to run a testing campaign of unprecedented depth on Intel and AMD CPUs. As a highlight, we discover two types of previously unknown speculative leaks (affecting string comparison and division) that have escaped previous manual and automatic analyses.

2023: Algorithm-assisted discovery of an intrinsic order among mathematical constants
Abstract: Significance Our research in the field of AI for science uncovered a mathematical structure that relates disparate mathematical constants through continued fraction formulas and offers an approach to proving their irrationality, a fundamental question in number theory. Enabling this discovery is a phenomenon termed factorial reduction, observed universally across formulas of numerous constants, from historical works to recent days. This research marks one of the most extensive automated discovery efforts in experimental mathematics, engaging thousands of volunteers to execute a massively parallel algorithm for more than two years. This effort showcases the impact of experimental mathematics on the broader community of math enthusiasts and encourages further explorations of this kind.

2023: Reducing The Virtual Memory Overhead in Nested Virtualization
Abstract: Virtualization has become a critical aspect of modern computing, and with the advent of virtualization-based containers, fast nested virtualization has become increasingly important. Nested virtualization is implemented by emulating virtualization capabilities to the guest host which can result in significant overhead. Another source of overheads in virtualization stems from the address translation mechanisms employed to implement virtualization, which usually causes a mix of slower address translation, frequently trapping guests, and loss of granularity in page tables. Our research focuses on using guest-managed physical memory with the use of per-VM memory tags for checking each VMs' access permissions.

2023: Nucleotide String Indexing using Range Matching
Abstract: The most common data-structures for genome indexing exhibit a fundamental trade-off between memory footprint and performance. We present Ranger, a new indexing technique that is both memory efficient and fast, and integrate it into the popular Minimap2 tool. Ranger achieves almost identical end-to-end performance as the original Minimap2, while occupying up to 1.7√ó and 1.2√ó less memory. With a limited memory capacity, Ranger achieves up to 4.3√ó performance speedup compares to current indexing techniques.

2023: NeuroLPM - Scaling Longest Prefix Match Hardware with Neural Networks
Abstract: Longest Prefix Match engines (LPM) are broadly used in computer systems and especially in modern network devices such as Network Interface Cards (NICs), switches and routers. However, existing LPM hardware fails to scale to millions of rules required by modern systems, is often optimized for specific applications, and thus is performance-sensitive to the structure of LPM rules.We describe NeuroLPM, a new architecture for multi-purpose LPM hardware that replaces queries in traditional memory-intensive trie- and hash-table data structures with inference in a lightweight Neural Network-based model, called RQRMI. NeuroLPM scales to millions of rules under small on-die SRAM budget and achieves stable, rule-structure-agnostic performance, allowing its use in a variety of applications. We solve several unique challenges when implementing RQRMI inference in hardware, including minimizing the amount of floating point computations while maintaining query correctness, and scaling the rule-set size while ensuring small, deterministic off-chip memory bandwidth.We prototype NeuroLPM in Verilog and evaluate it on real-world packet forwarding rule-sets and network traces. NeuroLPM offers substantial scalability benefits without any application-specific optimizations. For example, it is the only algorithm that can serve a 950K-large rule-set at an average of 196M queries per second with 4.5MB of SRAM, only within 2% of the best-case throughput of the state-of-the-art Tree Bitmap and SAIL on smaller rule-sets. With 2MB of SRAM, it reduces the DRAM bandwidth per query, the dominant performance factor, by up to 9√ó and 3√ó compared to the state-of-the-art.CCS CONCEPTS‚Ä¢ Hardware ‚Üí Networking hardware; ‚Ä¢ Networks ‚Üí Packet classification; ‚Ä¢ Computing methodologies ‚Üí Machine learn ing.

2023: SwitchVM: Multi-Tenancy for In-Network Computing
Abstract: We present SwitchVM, an in-switch virtual machine for reconfigurable match-action table programmable switches aimed at providing multi-tenant in-network computing.

2023: Neural Networks for Computer Systems
Abstract: We present the Range Query Recursive Model Index (RQRMI) data structure that trades memory accesses for computations in performance-critical systems that employ Range Matching.

2023: Translation Pass-Through for Near-Native Paging Performance in VMs
Abstract: Virtual machines (VMs) are used for consolidation, isolation, and provisioning in the cloud, but applications with large working sets are impacted by the overheads of memory address translation in VMs. Existing translation approaches incur non-trivial overheads: (i) nested paging has a worst-case latency that increases with page table depth; and (ii) par-avirtualized and shadow paging suffer from high hypervisor intervention costs when updating guest page tables. We describe translation pass-through (TPT), a new memory virtualization mechanism that achieves near-native performance. TPT enables VMs to control virtual memory translation from guest-virtual to host-physical addresses using one-dimensional page tables. At the same time, inter-VM isolation is enforced by the host by exploiting new hardware support for physical memory tagging in commodity CPUs. We prototype TPT by modifying the KVM/QEMU hyper-visor and enlightening the Linux guest. We evaluate it by emulating the memory tagging mechanism of AMD CPUs. Our conservative performance estimates show that TPT achieves native performance for real-world data center applications, with speedups of up to 2.4 √ó and 1.4 √ó over nested and shadow paging, respectively.

2023: AEX-Notify: Thwarting Precise Single-Stepping Attacks through Interrupt Awareness for Intel SGX Enclaves
Abstract: Intel ¬Æ Software Guard Extensions (Intel ¬Æ SGX) supports the creation of shielded enclaves within unprivileged processes. While enclaves are architecturally protected against malicious system software, Intel SGX‚Äôs privileged attacker model could potentially expose enclaves to new powerful side-channel attacks. In this paper, we consider hardware-software co-design countermeasures to an important class of single-stepping attacks that use privileged timer interrupts to precisely step through enclave execution exactly one instruction at a time, as supported, e.g., by the open-source SGX-Step framework. This is a powerful deterministic attack primitive that has been employed in a broad range of high-resolution Intel SGX attacks, but so far remains unmitigated. We propose AEX-Notify, a Ô¨Çexible hardware ISA extension that makes enclaves interrupt aware : enclaves can register a trusted handler to be run after an interrupt or exception. AEX-Notify can be used as a building block for implementing countermeasures against different types of interrupt-based attacks in software. With our primary goal to thwart deterministic single-stepping, we Ô¨Årst diagnose the underlying hardware behavior to determine the root cause that enables it. We then apply the learned insights to remove this root cause by building an efÔ¨Åcient software handler and constant-time dis-assembler to transparently determine and atomically prefetch the working set of the next enclave application instruction. The ISA extension we propose in this paper has been incorporated into a revised version of the Intel SGX speciÔ¨Åcation.

2022: SwiSh: Distributed Shared State Abstractions for Programmable Switches
Abstract: We design and evaluate SwiSh , a distributed shared state management layer for data-plane P4 programs. SwiSh enables running scalable stateful distributed network functions on programmable switches entirely in the data-plane. We explore several schemes to build a shared variable abstraction, which differ in consistency, performance, and in-switch implementation complexity. We introduce the novel Strong Delayed-Writes (SDW) protocol which offers consistent snapshots of shared data-plane objects with semantics known as ùëü -relaxed strong linearizability, enabling implementation of distributed concurrent sketches with precise error bounds. We implement strong, eventual, and SDW consistency protocols in ToÔ¨Åno switches, and compare their performance in microbenchmarks and three realistic network functions, NAT, DDoS detector, and rate limiter. Our results show that the distributed state management in the data plane is practical, and outperforms centralized solutions by up to four orders of magnitude in update throughput and replication latency.

2022: Slashing the disaggregation tax in heterogeneous data centers with FractOS
Abstract: Disaggregated heterogeneous data centers promise higher efficiency, lower total costs of ownership, and more flexibility for data-center operators. However, current software stacks can levy a high tax on application performance. Applications and OSes are designed for systems where local PCIe-connected devices are centrally managed by CPUs, but this centralization introduces unnecessary messages through the shared data-center network in a disaggregated system. We present FractOS, a distributed OS that is designed to minimize the network overheads of disaggregation in heterogeneous data centers. FractOS elevates devices to be first-class citizens, enabling direct peer-to-peer data transfers and task invocations among them, without centralized application and OS control. FractOS achieves this through: (1) new abstractions to express distributed applications across services and disaggregated devices, (2) new mechanisms that enable devices to securely interact with each other and other data-center services, (3) a distributed and isolated OS layer that implements these abstractions and mechanisms, and can run on host CPUs and SmartNICs. Our prototype shows that FractOS accelerates real-world heterogeneous applications by 47%, while reducing their network traffic by 3√ó.

2022: FlexDriver: a network driver for your accelerator
Abstract: We propose a new system design for connecting hardware and FPGA accelerators to the network, allowing the accelerator to directly control commodity Network Interface Cards (NICs) without using the CPU. This enables us to solve the key challenge of leveraging existing NIC hardware offloads such as virtualization, tunneling, and RDMA for accelerator networking. Our approach supports a diverse set of use cases, from direct network access for disaggregated accelerators to inline-acceleration of the network stack, all without the complex networking logic in the accelerator. To demonstrate the feasibility of this approach, we build FlexDriver (FLD), an on-accelerator hardware module that implements a NIC data-plane driver. Our main technical contribution is a mechanism that compresses the NIC control structures by two orders of magnitude, allowing FLD to achieve high networking scalability with low die area cost and no bandwidth interference with the accelerator logic. The prototype for NVIDIA Innova-2 FPGA SmartNICs showcases our design‚Äôs utility for three different accelerators: a disaggregated LTE cipher, an IP-defragmentation inline accelerator, and an IoT cryptographic-token authentication offload. These accelerators reach 25 Gbps line rate and leverage the NIC for RDMA processing, VXLAN tunneling, and traffic shaping without CPU involvement.

2022: Securing Access to Untrusted Services From TEEs with GateKeeper
Abstract: Applications running in Trusted Execution Environments (TEEs) commonly use untrusted external services such as host File System. Adversaries may maliciously alter the normal service behavior to trigger subtle application bugs that would have never occurred under correct service operation, causing data leaks and integrity violations. Unfortunately, existing manual protections are incomplete and ad-hoc, whereas formally-veriÔ¨Åed ones require special expertise. We introduce GateKeeper , a framework to develop mitiga-tions and vulnerability checkers for such attacks by leveraging lightweight formal models of untrusted services. With the attack seen as a violation of a services‚Äô functional correctness, GateKeeper takes a novel approach to develop a comprehensive model of a service without requiring formal methods expertise. We harness available testing suites routinely used in service development to tighten the model to known correct service implementation. GateKeeper uses the resulting model to automatically generate (1) a correct-by-construction runtime service validator in C that is linked with a trusted application and guards each service invocation to conform to the model; and (2) a targeted model-driven vulnerability checker for analyzing black-box applications. We evaluate GateKeeper on Intel SGX enclaves. We develop comprehensive models of a POSIX Ô¨Åle system and OS synchronization primitives while using thousands of existing test suites to tighten their models to the actual Linux implementations. We generate the validator and integrate it with Graphene-SGX, and successfully protect unmodiÔ¨Åed Memcached and SQLite with negligible overheads. The generated vulnerability checker detects novel vulnerabilities in the Graphene-SGX protection layer and production applications.

2022: Reconsidering OS memory optimizations in the presence of disaggregated memory
Abstract: Tiered memory systems introduce an additional memory level with higher-than-local-DRAM access latency and require sophisticated memory management mechanisms to achieve cost-efficiency and high performance. Recent works focus on byte-addressable tiered memory architectures which offer better performance than pure swap-based systems. We observe that adding disaggregation to a byte-addressable tiered memory architecture requires important design changes that deviate from the common techniques that target lower-latency non-volatile memory systems. Our comprehensive analysis of real workloads shows that the high access latency to disaggregated memory undermines the utility of well-established memory management optimizations Based on these insights, we develop HotBox ‚Äì a disaggregated memory management subsystem for Linux that strives to maximize the local memory hit rate with low memory management overhead. HotBox introduces only minor changes to the Linux kernel while outperforming state-of-the-art systems on memory-intensive benchmarks by up to 2.25√ó.

2021: A readahead prefetcher for GPU file system layer
Abstract: GPUs are broadly used in I/O-intensive big data applications. Prior works demonstrate the benefits of using GPU-side file system layer, GPUfs, to improve the GPU performance and programmability in such workloads. However, GPUfs fails to provide high performance for a common I/O pattern where a GPU is used to process a whole data set sequentially. In this work, we propose a number of system-level optimizations to improve the performance of GPUfs for such workloads. We perform an in-depth analysis of the interplay between the GPU I/O access pattern, CPU-GPU PCIe transfers and SSD storage, and identify the main bottlenecks. We propose a new GPU I/O readahead prefetcher and a GPU page cache replacement mechanism to resolve them. The GPU I/O readahead prefetcher achieves more than $2\times$ (geometric mean) higher bandwidth in a series of microbenchmarks compared to the original GPUfs. Furthermore, we evaluate the system on 14 applications derived from the RODINIA, PARBOIL and POLYBENCH benchmark suites. Our prefetching mechanism improves their execution time by up to 50% and their I/O bandwidth by 82% compared to the traditional CPU-only data transfer techniques.

2021: the 2021 USENIX
Abstract: Fine-tuning is an increasingly common technique that leverages transfer learning to dramatically expedite the training of huge, high-quality models. Critically, Ô¨Åne-tuning holds the potential to make giant state-of-the-art models pre-trained on high-end super-computing-grade systems readily available for users that lack access to such costly resources. Unfortunately, this potential is still difÔ¨Åcult to realize because the models often do not Ô¨Åt in the memory of a single commodity GPU, making Ô¨Åne-tuning a challenging problem. We present FTPipe, a system that explores a new dimension of pipeline model parallelism, making multi-GPU execution of Ô¨Åne-tuning tasks for giant neural networks readily accessible on commodity hardware. A key idea is a novel approach to model partitioning and task allocation, called Mixed-pipe. Mixed-pipe partitions the model into arbitrary computational blocks rather than layers, and relaxes the model topology constraints when assigning blocks to GPUs, allowing non-adjacent blocks to be executed on the same GPU. More Ô¨Çexi-ble partitioning affords a much better balance of the compute-and memory-load on the GPUs compared to prior works, yet does not increase the communication overheads. Moreover, and perhaps surprisingly, when applied to asynchronous training, Mixed-pipe has negligible or no effect on the end-to-end accuracy of Ô¨Åne-tuning tasks despite the addition of pipeline stages. Our extensive experiments on giant state-of-the-art NLP models (BERT-340M, GPT2-1.5B, and T5-3B) show that FT-Pipe achieves up to 3 √ó speedup and state-of-the-art accuracy when Ô¨Åne-tuning giant transformers with billions of parameters. These models require from 12GB to 59GB of GPU memory, and FTPipe executes them on 8 commodity RTX2080-Ti GPUs, each with 11GB memory and standard PCIe

2021: Revizor: Fuzzing for Leaks in Black-box CPUs
Abstract: Shared microarchitectural state has become a prime target for side-channel attacks that leverage timing measurements to leak information across security domains. Combined with speculative execution, they cause vulnerabilities like Spectre and Meltdown. Such vulnerabilities often stay undetected for a long time because we lack the tools for systematic testing of CPUs against them. In this paper, we propose an approach to automatically detect microarchitectural information leakage in commercial black-box CPUs. We base our approach on speculation contracts, which we employ to specify the permitted side effects of program execution on the microarchitectural state. We propose a technique, called Model-based Relational Fuzzing (MRF), that enables testing of CPUs against these specifications. We implement MRF in a fuzzing framework called Revizor, and showcase its effectiveness on real Intel x86 CPUs: It automatically detects violations of a rich set of contracts, or indicates their absence. A highlight of our findings is that Revizor managed to automatically surface Spectre, MDS, and LVI by fuzzing against increasingly liberal contracts.

