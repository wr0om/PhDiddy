Recent papers for Katzenelson Jacob:

2002: Numerical Weather Prediction on the Supercomputer Toolkit
Abstract: None

1999: Flow-Dire ted Lightweight Closure ConversionJEFFREY
Abstract: None

1999: Symbolic-numeric circuit analysis or symbolic circuit analysis with online approximations
Abstract: This paper describes a method for obtaining simplified symbolic expressions for the transfer functions of analog electronic circuits and, in particular, analog transistor circuits. The circuits are modeled as linear lumped RLC electrical networks with dependent sources. Both the frequency and the values of the components appear as symbols in the transfer functions. The simplification is done by symbolic computation with approximations taking place during the computation. The method has three essential parts of the method. (1) The user partitions the network to modules satisfying mismatch conditions (to the first approximation). The transfer functions of each module are evaluated and the results are combined to form the transfer functions of the network. (2) A nominal value is associated with each component; R1/spl Gt/R2 if and only if the absolute nominal value of R1 is "much larger" than the absolute nominal value of R2; When R1+R2 is evaluated, R1 is returned if R1/spl Gt/R2. The relation /spl Gt/ is defined between functions of the frequency as well as between values. (3) Whenever possible, polynomials are represented as a product of lower order polynomials. When the lower order polynomials are of order one or two, poles or zeros are available in symbolic form. This paper presents transistor circuit examples computed by the SCHEME program which implements the method.

1996: Elastic-plastic flow simulation using the Supercomputer Toolkit
Abstract: This article describes the first application program run on Technion's Supercomputer Toolkit. The program simulates the behavior of a high speed metal projectile colliding with a rigid wall. The velocity of the projectile is high enough to cause plastic deformations. After briefly describing the Toolkit and the elastic-plastic flow problem this article is concerned with the relation between the Toolkit architecture and the properties of the elastic-plastic simulation. The main conclusions are that the parallelization process is straightforward, and the obtained speedup, for a small number of processors, is about equal to the number of processors. The first item is a result of the elastic-plastic simulation computational structure, which can be viewed as computations done within cells that communicate mainly with their nearest neighbors. This structure is parallelized easily and maps simply and readily to a Toolkit parallel computing network. The computation structure leads to a fair amount of local communication among Toolkit processors, but to only a modest amount of global communication. The Toolkit's local communication is exceedingly fast and the time for local communication is independent of the network size, which explains the second conclusion and fits the requirement of large elastic-plastic problems.

1995: A NETWORK CHARGE-ORIENTED MOS TRANSISTOR MODEL
Abstract: The MOS transistor physical model as described in Ref. 3 is presented here as a network model. The goal is to obtain an accurate model, suitable for simulation, free from certain problems reported in the literature,13 and conceptually as simple as possible. To achieve this goal the original model had to be extended and modified. The paper presents the derivation of the network model from physical equations, including the corrections which are required for simulation and which compensate for simplifications introduced in the original physical model. Our intrinsic MOS model consists of three nonlinear voltage-controlled capacitors and a dependent current source. The charges of the capacitors and the current of the current source are functions of the voltages Vgs, Vbs, and Vds. The complete model consists of the intrinsic model plus the parasitics. The apparent simplicity of the model is a result of hiding information in the characteristics of the nonlinear components. The resulted network model has been checked by simulation and analysis. It is shown that the network model is suitable for simulation: It is defined for any value of the voltages; the functions involved are continuous and satisfy the Lipschitz conditions with no jumps at region boundaries; Derivatives have been computed symbolically and are available for use by the Newton-Raphson method. The model’s functions can be measured from the terminals. It is also shown that small channel effects can be included in the model. Higher frequency effects can be modeled by using a network consisting of several sections of the basic lumped model. Future plans include a detailed comparison of the network model with models such as SPICE level 3 and a comparison of the multi-section higher frequency model with experiments.

1992: Type matching, type-graphs, and the Schanuel conjecture
Abstract: This work considers type systems that are defined by type-graphs (tgraphs), which are rooted directed graphs with order among the edges leaving each node. Tgraphs are uniquely mapped into polynomials which, in turn, are each evaluated at a special point to yield an irrational number named the tgraph's magic number. This special point is chosen using the Schanuel conjecture. It is shown that each tgraph can be uniquely represented by this magic number; namely, types are equal if and only if the corresponding magic numbers are equal. Since irrational numbers require infinite precision, the algorithm for generating magic numbers is carried out using a double-precision floating-point approximation. This approximation is viewed as a hashing scheme, mapping the infinite domain of the irrational numbers into finite computer words. The proposed hashing scheme was investigated experimentally, with the conclusion that it is a good and practical hashing method. In tests involving over a million randomly chosen tgraphs, we have not encountered a single collision. We conclude that this method for representation and management of types is practical, and offers novel possibilities for enforcing strict type matching at link time among separately compiled modules.

1991: The Supercomputer Toolkit: A General Framework for Special- purpose Computing
Abstract: The Toolkit is a family of hardware modules (processors, memory, interconnect, and input-output devices) and a collection of software modules (compilers, simulators, scientific libraries, and high-level front ends) from which high-performance special-purpose computers can be easily configured and programmed. The hardware modules are intended to be standard, reusable parts. These are combined by means of a user- reconfigurable, static interconnect technology. The Toolkit''s software support, based on novel compilation techniques, produces extremely high- performance numerical code from high-level language input, and will eventually automatically configure hardware modules for particular applications.

1991: Supercomputer toolkit: A general framework for special purpose computing. Memorandum report
Abstract: The Toolkit is a family of hardware modules (processors, memory, interconnect, and input-output devices) and a collection of software modules (compilers, simulators, scientific libraries, and high-level front ends) from which high-performance special-purpose computers can be easily configured and programmed. The hardware modules are intended to be standard, reusable parts. These are combined by means of a user-reconfigurable, static interconnect technology. The Toolkit's software support, based on novel compilation techniques, produces extremely high-performance numerical code from high-level language input, and will eventually automatically configure hardware modules for particular applications.

1990: Supercomputer Toolkit and its applications. Memorandum report
Abstract: None

1990: The Supercomputer Toolkit and its applications
Abstract: The Supercomputer Toolkit, a proposed family of standard hardware and software components from which special-purpose machines can be easily configured, is introduced. Using the Toolkit, a scientist or an engineer, starting with a suitable computational problem, will be able to readily configure a special-purpose multiprocessor that attains supercomputer-class performance on that problem, at a fraction of the cost of a general purpose supercomputer. Some applications, the hardware, and the software are described.<<ETX>>

1989: EC-Pl: an Integrated Circuit Layout Program in the Enhanced C Language
Abstract: We have designed and implemented a system called EC-PI that automatically lays out integrated circuits. EC-PI provides a test bed for layout algorithms. It is also designed as an experiment in using very high level programming languages for the implementation of such algorithms. This paper reports the structure and performance of the system its well as our conclusions from developing and using it. FGPI partitions the layout process into several steps: cell placement, pad placement, routing power and ground nets in a single-metal layer, channel definition, global routing, crossing placement, and switch-box routing. This partition follows the MIT PI Placemant and Interconnat system [13], however, the implementation of the steps differs and includes several novel techniques. Among these techniques are a new cost function for cell placement, a simple and efficient algorithm for pad placing, a new algorithm for channel definition, and a new implementation of PI's global routing algorithm. Several layout examples illustrating the performance of the system are given in appendix A. The layout program is implemented using the Enhanced C (EC) programming language. The work presents conclusions related to both the layout process and use of the EC language.

1989: Intelligence in scientific computing
Abstract: The authors discuss the development of intelligent techniques appropriate for the automatic preparation, execution, and control of numerical experiments.

1989: Computational structure of the N-body problem
Abstract: This work considers tree algorithms for the N-body problem where the number of particles is on the order of a million. The main concern of this work is the organization and performance of these computations on parallel computers.This work introduces a formulation of the N-body problem as a set of recursive equations based on a few elementary functions. It is shown that both the algorithm of Barnes–Hut and that of Greengard–Rokhlin satisfy these equations using different elementary functions. The recursive formulation leads directly to a computational structure in the form of a pyramid-like graph, where each vertex is a process, and each arc a communication link.The pyramid is mapped to three different processor configurations: (1) a pyramid of processors corresponding to the processes pyramid graph; (2) a hypercube of processors, e.g., a connection-machine-like architecture; and (3) a rather small array, e.g., $2 \times 2 \times 2$, of processors faster than the ones considered in (1) and (2) above.The main conclusion is that simulations of this size can be performed on any of the three architectures in reasonable time. Approximately 24 seconds per timestep is the estimate for a million equally distributed particles using the Greengard-Rokhlin algorithm on the CM-2 connection machine. The smaller array of processors is quite competitive in performance.

1988: Software structuring principles for VLSI CAD
Abstract: It is argued that systems should be designed for reusability by anticipating change. This goal can be achieved by designing the software by layers of problem-oriented languages, which are implemented by suitably extending a base language. A language layer rarely needs to be adapted to changes, only the application (i.e. algorithm) has to be changed. The authors illustrate this methodology with respect to VLSI CAD programs and a particular language layer: a language for handling networks. Such a language consists of a base language (EC or Lisp) plus data types, operations and control structures that are relevant to network problems. The network language is but one of several languages used; other languages used deal with sets, two-dimensional layout structures, waveforms, etc. The discussion of the network language illustrates this technique.<<ETX>>

1988: The LISP experience
Abstract: This article exhibits programs that illustrate the power and simplicity of Lisp as a language for expressing the design and organization of com­ putational systems. Lisp, created in the late 1950s (McCarthy 1960), is, after FORTRAN, the oldest computer language in widespread use today. Despite its age, Lisp is still a "modern" language that continues to adapt to encompass the newest ideas in programming (Steele 1984). Over the past 30 years, Lisp has remained the language of choice for designing and prototyping complex programs, particularly in symbolic processing and artificial intelligence applications. Lisp's vitality is rooted in its power to express robust designs for com­ putational systems. A design for a system is robust if small changes in the system requirements can be accomplished by small changes in the design. One way to enhance the robustness of a design is to express it in terms of a natural representation of the problem domain, where each relevant entity in the problem domain is modeled by a computational entity and each relation or operation in the problem domain is modeled by a computational relation or function. With such an organization, small changes in the configuration of the problem domain can be accommodated by small changes in the configuration of the computational model. For a programming language to support such natural representations, it must allow programmers the freedom to construct appropriate com­ putational abstractions to represent the elements of problem domains. Lisp provides this freedom in two essential ways. First, Lisp does not limit procedural abstraction by placing arbitrary restrictions on passing procedures as arguments, returning procedures as values, and including

1987: Debugging programs that use macro‐oriented data abstractions
Abstract: Enhanced C (EC) is a set‐oriented, extensible, C‐like language. EC uses data abstractions to define new types. These data abstractions, called clusters, are macro‐like devices that perform substitution on the typed syntax tree. Debugging programs that use clusters raise problems that are not encountered in ordinary programming languages. At compile time there is a need to determine and report whether the macro expansion will result in a legal program before this expansion actually takes place. At run‐time the problems are how to account for replaced statements and how to handle variables whose types have been established by the clusters, variables that disappear, or variables whose names have been changed.

1986: VLSI Simulation and Data Abstractions
Abstract: Multilevel simulation is a technique for specifying and testing VLSI chip designs. This paper compares three methods of implementing such simulators: (a) Programming using a high level language that does not have data abstraction, e.g., Pascal. (b) Programming with procedure oriented data abstractions as in Simula [3] or CLU [4]. (c) Programming with enhanced C's (EC) [2] data abstractions that are macro oriented. LIST -- a generator for simulation programs based on the data abstraction technique is described briefly. It is concluded that the use of data abstractions offers structure, hierarchy and simplicity. The use of EC has the additional advantage of run-time efficiency.

1983: Introduction to enhanced C (EC)
Abstract: The main goals of EC are to design a set‐oriented, extensible, C‐like language and to build a translator for this language that produces efficient run‐time code.

1983: Higher level programming and data abstractions—a case study using enhanced C
Abstract: In this work the topological sort algorithm is programmed in EC, taking advantage of EC's set‐oriented features and its macro‐like data abstractions. The main issue was whether efficient run‐time code could be achieved while maintaining program structure and readability.

1979: Clusters and Dialogues for Set Implementations
Abstract: The extensible set language (ESL) uses clusters for extending the set oriented language and for mapping the sets and their operators into base language data structure and operators. The main difficulty in this mapping is the dependency among clusters, i.e., the use of one data structure requires the modification or the constraint of another. The dialogue is a compile time procedure which is part of a cluster. It manipulates data structures and enables each cluster to appear independent; it also enables the user to choose one of a family of data implementations defmed by the cluster. Thus the dialogue makes it possible for a data structure to appear as a building block which can be used simply and flexibly.

