Recent papers for Keren Censor-Hillel:

2024: Fast Approximate Counting of Cycles
Abstract: We consider the problem of approximate counting of triangles and longer fixed length cycles in directed graphs. For triangles, T\v{e}tek [ICALP'22] gave an algorithm that returns a $(1 \pm \eps)$-approximation in $\tilde{O}(n^\omega/t^{\omega-2})$ time, where $t$ is the unknown number of triangles in the given $n$ node graph and $\omega<2.372$ is the matrix multiplication exponent. We obtain an improved algorithm whose running time is, within polylogarithmic factors the same as that for multiplying an $n\times n/t$ matrix by an $n/t \times n$ matrix. We then extend our framework to obtain the first nontrivial $(1 \pm \eps)$-approximation algorithms for the number of $h$-cycles in a graph, for any constant $h\geq 3$. Our running time is \[\tilde{O}(\mathsf{MM}(n,n/t^{1/(h-2)},n)), \textrm{the time to multiply } n\times \frac{n}{t^{1/(h-2)}} \textrm{ by } \frac{n}{t^{1/(h-2)}}\times n \textrm{ matrices}.\] Finally, we show that under popular fine-grained hypotheses, this running time is optimal.

2024: Faster Cycle Detection in the Congested Clique
Abstract: We provide a fast distributed algorithm for detecting $h$-cycles in the \textsf{Congested Clique} model, whose running time decreases as the number of $h$-cycles in the graph increases. In undirected graphs, constant-round algorithms are known for cycles of even length. Our algorithm greatly improves upon the state of the art for odd values of $h$. Moreover, our running time applies also to directed graphs, in which case the improvement is for all values of $h$. Further, our techniques allow us to obtain a triangle detection algorithm in the quantum variant of this model, which is faster than prior work. A key technical contribution we develop to obtain our fast cycle detection algorithm is a new algorithm for computing the product of many pairs of small matrices in parallel, which may be of independent interest.

2024: Near-Optimal Resilient Labeling Schemes
Abstract: Labeling schemes are a prevalent paradigm in various computing settings. In such schemes, an oracle is given an input graph and produces a label for each of its nodes, enabling the labels to be used for various tasks. Fundamental examples in distributed settings include distance labeling schemes, proof labeling schemes, advice schemes, and more. This paper addresses the question of what happens in a labeling scheme if some labels are erased, e.g., due to communication loss with the oracle or hardware errors. We adapt the notion of resilient proof-labeling schemes of Fischer, Oshman, Shamir [OPODIS 2021] and consider resiliency in general labeling schemes. A resilient labeling scheme consists of two parts -- a transformation of any given labeling to a new one, executed by the oracle, and a distributed algorithm in which the nodes can restore their original labels given the new ones, despite some label erasures. Our contribution is a resilient labeling scheme that can handle $F$ such erasures. Given a labeling of $\ell$ bits per node, it produces new labels with multiplicative and additive overheads of $O(1)$ and $O(\log(F))$, respectively. The running time of the distributed reconstruction algorithm is $O(F+(\ell\cdot F)/\log{n})$ in the \textsf{Congest} model. This improves upon what can be deduced from the work of Bick, Kol, and Oshman [SODA 2022], for non-constant values of $F$. It is not hard to show that the running time of our distributed algorithm is optimal, making our construction near-optimal, up to the additive overhead in the label size.

2024: On Distributed Computation of the Minimum Triangle Edge Transversal
Abstract: The distance of a graph from being triangle-free is a fundamental graph parameter, counting the number of edges that need to be removed from a graph in order for it to become triangle-free. Its corresponding computational problem is the classic minimum triangle edge transversal problem, and its normalized value is the baseline for triangle-freeness testing algorithms. While triangle-freeness testing has been successfully studied in the distributed setting, computing the distance itself in a distributed setting is unknown, to the best of our knowledge, despite being well-studied in the centralized setting. This work addresses the computation of the minimum triangle edge transversal in distributed networks. We show with a simple warm-up construction that this is a global task, requiring $\Omega(D)$ rounds even in the $\mathsf{LOCAL}$ model with unbounded messages, where $D$ is the diameter of the network. However, we show that approximating this value can be done much faster. A $(1+\epsilon)$-approximation can be obtained in $\text{poly}\log{n}$ rounds, where $n$ is the size of the network graph. Moreover, faster approximations can be obtained, at the cost of increasing the approximation factor to roughly 3, by a reduction to the minimum hypergraph vertex cover problem. With a time overhead of the maximum degree $\Delta$, this can also be applied to the $\mathsf{CONGEST}$ model, in which messages are bounded. Our key technical contribution is proving that computing an exact solution is ``as hard as it gets'' in $\mathsf{CONGEST}$, requiring a near-quadratic number of rounds. Because this problem is an edge selection problem, as opposed to previous lower bounds that were for node selection problems, major challenges arise in constructing the lower bound, requiring us to develop novel ingredients.

2023: Near-Optimal Fault Tolerance for Efficient Batch Matrix Multiplication via an Additive Combinatorics Lens
Abstract: Fault tolerance is a major concern in distributed computational settings. In the classic master-worker setting, a server (the master) needs to perform some heavy computation which it may distribute to $m$ other machines (workers) in order to speed up the time complexity. In this setting, it is crucial that the computation is made robust to failed workers, in order for the master to be able to retrieve the result of the joint computation despite failures. A prime complexity measure is thus the \emph{recovery threshold}, which is the number of workers that the master needs to wait for in order to derive the output. This is the counterpart to the number of failed workers that it can tolerate. In this paper, we address the fundamental and well-studied task of matrix multiplication. Specifically, our focus is on when the master needs to multiply a batch of $n$ pairs of matrices. Several coding techniques have been proven successful in reducing the recovery threshold for this task, and one approach that is also very efficient in terms of computation time is called \emph{Rook Codes}. The previously best known recovery threshold for batch matrix multiplication using Rook Codes is $O(n^{\log_2{3}})=O(n^{1.585})$. Our main contribution is a lower bound proof that says that any Rook Code for batch matrix multiplication must have a recovery threshold that is at least $\omega(n)$. Notably, we employ techniques from Additive Combinatorics in order to prove this, which may be of further interest. Moreover, we show a Rook Code that achieves a recovery threshold of $n^{1+o(1)}$, establishing a near-optimal answer to the fault tolerance of this coding scheme.

2023: Correction to: Distributed computations in fully-defective networks
Abstract: None

2022: Distributed computations in fully-defective networks
Abstract: None

2022: Deterministic Near-Optimal Distributed Listing of Cliques
Abstract: The importance of classifying connections in large graphs has been the motivation for a rich line of work on distributed subgraph finding that has led to exciting recent breakthroughs. A crucial aspect that remained open was whether deterministic algorithms can be as efficient as their randomized counterparts, where the latter are known to be tight up to polylogarithmic factors. We give deterministic distributed algorithms for listing cliques of size p in n1 - 2/p + o(1) rounds in the Congest model. For triangles, our n1/3+o(1) round complexity improves upon the previous state of the art of n2/3+o(1) rounds [Chang and Saranurak, FOCS 2020]. For cliques of size p ≥ 4, ours are the first non-trivial deterministic distributed algorithms. Given known lower bounds, for all values p ≥ 3 our algorithms are tight up to a no(1) subpolynomial factor, which comes from the deterministic routing procedure we use.

2022: Quantum Distributed Algorithms for Detection of Cliques
Abstract: The possibilities offered by quantum computing have drawn attention in the distributed computing community recently, with several breakthrough results showing quantum distributed algorithms that run faster than the fastest known classical counterparts, and even separations between the two models. A prime example is the result by Izumi, Le Gall, and Magniez [STACS 2020], who showed that triangle detection by quantum distributed algorithms is easier than triangle listing, while an analogous result is not known in the classical case. In this paper we present a framework for fast quantum distributed clique detection. This improves upon the state-of-the-art for the triangle case, and is also more general, applying to larger clique sizes. Our main technical contribution is a new approach for detecting cliques by encapsulating this as a search task for nodes that can be added to smaller cliques. To extract the best complexities out of our approach, we develop a framework for nested distributed quantum searches, which employ checking procedures that are quantum themselves. Moreover, we show a circuit-complexity barrier on proving a lower bound of the form $\Omega(n^{3/5+\epsilon})$ for $K_p$-detection for any $p \geq 4$, even in the classical (non-quantum) distributed CONGEST setting.

2022: 2022 Principles of Distributed Computing Doctoral Dissertation Award
Abstract: Many exceptionally high-quality doctoral dissertations were submitted for the 2022 Principles of Distributed Computing Doctoral Dissertation Award. After careful long deliberation, the award committee decided to share the award among two: ⋅ Dr. Naama Ben-David for her dissertation ''Theoretical Foundations for Practical Concurrent and Distributed Computation.'' ⋅ Dr. Manuela Fischer for her dissertation ''Local Algorithms for Classic Graph Problems.''

2022: Distributed Subgraph Finding: Progress and Challenges (Invited Talk)
Abstract: This is a survey of the exciting recent progress made in understanding the complexity of distributed subgraph finding problems. It overviews the results and techniques for assorted variants of subgraph finding problems in various models of distributed computing, and states intriguing open questions. This version contains some updates over the ICALP 2021 version, and I will try to keep updating it as additional progress is made.

2021: On Sparsity Awareness in Distributed Computations
Abstract: We extract a core principle that underlies seemingly different fundamental distributed settings, which is that sparsity awareness may induce faster algorithms for core problems in these settings. To leverage this, we establish a new framework by developing an intermediate auxiliary model which is weak enough to be successfully simulated in the classic congest model given low mixing time, as well as in the recently introduced hybrid model. We prove that despite imposing harsh restrictions, this artificial model allows balancing massive data transfers with a maximal utilization of bandwidth. We then exemplify the power we gain from our methods, by deriving fast shortest-paths algorithms which greatly improve upon the state-of-the-art.

2021: 2021 Edsger W. Dijkstra Prize in Distributed Computing
Abstract: a foundational

2021: Locally Checkable Labelings with Small Messages
Abstract: A rich line of work has been addressing the computational complexity of locally checkable labelings ( LCL s), illustrating the landscape of possible complexities. In this paper, we study the landscape of LCL complexities under bandwidth restrictions. Our main results are twofold. First, we show that on trees, the CONGEST complexity of an LCL problem is asymptotically equal to its complexity in the LOCAL model. An analog statement for non- LCL problems is known to be false. Second, we show that for general graphs this equivalence does not hold, by providing an LCL problem for which we show that it can be solved in O (log n ) rounds in the LOCAL model, but requires ˜Ω( n 1 / 2 ) rounds in the CONGEST model. we show that there is a broad family of graph problems – locally checkable labelings or LCL s in short – in which the two models of computing have exactly the same expressive power in trees (up to constant factors): if a locally checkable labeling problem Π can be solved in trees in T ( n ) communication rounds in the LOCAL model, it can be solved in O ( T ( n )) rounds also in the CONGEST model. We also show that this is no longer the case if we switch from trees to general graphs:

2021: Distributed Vertex Cover Reconfiguration
Abstract: Reconfiguration schedules, i.e., sequences that gradually transform one solution of a problem to another while always maintaining feasibility, have been extensively studied. Most research has dealt with the decision problem of whether a reconfiguration schedule exists, and the complexity of finding one. A prime example is the reconfiguration of vertex covers. We initiate the study of batched vertex cover reconfiguration, which allows to reconfigure multiple vertices concurrently while requiring that any adversarial reconfiguration order within a batch maintains feasibility. The latter provides robustness, e.g., if the simultaneous reconfiguration of a batch cannot be guaranteed. The quality of a schedule is measured by the number of batches until all nodes are reconfigured, and its cost, i.e., the maximum size of an intermediate vertex cover. To set a baseline for batch reconfiguration, we show that for graphs belonging to one of the classes $\{\mathsf{cycles, trees, forests, chordal, cactus, even\text{-}hole\text{-}free, claw\text{-}free}\}$, there are schedules that use $O(\varepsilon^{-1})$ batches and incur only a $1+\varepsilon$ multiplicative increase in cost over the best sequential schedules. Our main contribution is to compute such batch schedules in $O(\varepsilon^{-1}\log^* n)$ distributed time, which we also show to be tight. Further, we show that once we step out of these graph classes we face a very different situation. There are graph classes on which no efficient distributed algorithm can obtain the best (or almost best) existing schedule. Moreover, there are classes of bounded degree graphs which do not admit any reconfiguration schedules without incurring a large multiplicative increase in the cost at all.

2021: Near-Optimal Scheduling in the Congested Clique
Abstract: None

2021: Fault Tolerant Max-Cut
Abstract: In this work, we initiate the study of fault tolerant Max Cut, where given an edge-weighted undirected graph $G=(V,E)$, the goal is to find a cut $S\subseteq V$ that maximizes the total weight of edges that cross $S$ even after an adversary removes $k$ vertices from $G$. We consider two types of adversaries: an adaptive adversary that sees the outcome of the random coin tosses used by the algorithm, and an oblivious adversary that does not. For any constant number of failures $k$ we present an approximation of $(0.878-\epsilon)$ against an adaptive adversary and of $\alpha_{GW}\approx 0.8786$ against an oblivious adversary (here $\alpha_{GW}$ is the approximation achieved by the random hyperplane algorithm of [Goemans-Williamson J. ACM `95]). Additionally, we present a hardness of approximation of $\alpha_{GW}$ against both types of adversaries, rendering our results (virtually) tight. The non-linear nature of the fault tolerant objective makes the design and analysis of algorithms harder when compared to the classic Max Cut. Hence, we employ approaches ranging from multi-objective optimization to LP duality and the ellipsoid algorithm to obtain our results.

2021: Special Section on the 48th Annual ACM Symposium on Theory of Computing (STOC 2016)
Abstract: None

2021: Fast Distributed Algorithms for Girth, Cycles and Small Subgraphs
Abstract: In this paper we give fast distributed graph algorithms for detecting and listing small subgraphs, and for computing or approximating the girth. Our algorithms improve upon the state of the art by polynomial factors, and for girth, we obtain an constant-time algorithm for additive +1 approximation in the Congested Clique, and the first parametrized algorithm for exact computation in CONGEST. In the Congested Clique, we develop a technique for learning small neighborhoods, and apply it to obtain an $O(1)$-round algorithm that computes the girth with only an additive +1 error. Next, we introduce a new technique (the partition tree technique) allowing for efficiently and deterministically listing all copies of any subgraph, improving upon the state-of the-art for non-dense graphs. We give two applications of this technique: First we show that for constant $k$, $C_{2k}$-detection can be solved in $O(1)$ rounds in the Congested Clique, improving on prior work which used matrix multiplication and had polynomial round complexity. Second, we show that in triangle-free graphs, the girth can be exactly computed in time polynomially faster than the best known bounds for general graphs. In CONGEST, we describe a new approach for finding cycles, and apply it in two ways: first we show a fast parametrized algorithm for girth with round complexity $\tilde{O}(\min(g\cdot n^{1-1/\Theta(g)},n))$ for any girth $g$; and second, we show how to find small even-length cycles $C_{2k}$ for $k = 3,4,5$ in $O(n^{1-1/k})$ rounds, which is a polynomial improvement upon the previous running times. Finally, using our improved $C_6$-freeness algorithm and the barrier on proving lower bounds on triangle-freeness of Eden et al., we show that improving the current $\tilde\Omega(\sqrt{n})$ lower bound for $C_6$-freeness of Korhonen et al. by any polynomial factor would imply strong circuit complexity lower bounds.

2020: Distance Computations in the Hybrid Network Model via Oracle Simulations
Abstract: The Hybrid network model was introduced in [Augustine et al., SODA '20] for laying down a theoretical foundation for networks which combine two possible modes of communication: One mode allows high-bandwidth communication with neighboring nodes, and the other allows low-bandwidth communication over few long-range connections at a time. This fundamentally abstracts networks such as hybrid data centers, and class-based software-defined networks. 
Our technical contribution is a \emph{density-aware} approach that allows us to simulate a set of \emph{oracles} for an overlay skeleton graph over a Hybrid network. 
As applications of our oracle simulations, with additional machinery that we provide, we derive fast algorithms for fundamental distance-related tasks. One of our core contributions is an algorithm in the Hybrid model for computing \emph{exact} weighted shortest paths from $\tilde O(n^{1/3})$ sources which completes in $\tilde O(n^{1/3})$ rounds w.h.p. This improves, in both the runtime and the number of sources, upon the algorithm of [Kuhn and Schneider, PODC '20], which computes shortest paths from a single source in $\tilde O(n^{2/5})$ rounds w.h.p. 
We additionally show a 2-approximation for weighted diameter and a $(1+\epsilon)$-approximation for unweighted diameter, both in $\tilde O(n^{1/3})$ rounds w.h.p., which is comparable to the $\tilde \Omega(n^{1/3})$ lower bound of [Kuhn and Schneider, PODC '20] for a $(2-\epsilon)$-approximation for weighted diameter and an exact unweighted diameter. We also provide fast distance \emph{approximations} from multiple sources and fast approximations for eccentricities.

