Recent papers for Zohar Yakhini:

2025: The Labeled Coupon Collector Problem with Random Sample Sizes and Partial Recovery
Abstract: We extend the Coupon Collector's Problem (CCP) and present a novel generalized model, referred as the k-LCCP problem, where one is interested in recovering a bipartite graph with a perfect matching, which represents the coupons and their matching labels. We show two extra-extensions to this variation: the heterogeneous sample size case (K-LCCP) and the partly recovering case.

2025: Constrained Coding for Composite DNA: Channel Capacity and Efficient Constructions
Abstract: Composite DNA is a recent novel method to increase the information capacity of DNA-based data storage above the theoretical limit of 2 bits/symbol. In this method, every composite symbol does not store a single DNA nucleotide but a mixture of the four nucleotides in a predetermined ratio. By using different mixtures and ratios, the alphabet can be extended to have much more than four symbols in the naive approach. While this method enables higher data content per synthesis cycle, potentially reducing the DNA synthesis cost, it also imposes significant challenges for accurate DNA sequencing since the base-level errors can easily change the mixture of bases and their ratio, resulting in changes to the composite symbols. With this motivation, we propose efficient constrained coding techniques to enforce the biological constraints, including the runlength-limited constraint and the GC-content constraint, into every DNA synthesized oligo, regardless of the mixture of bases in each composite letter and their corresponding ratio. Our goals include computing the capacity of the constrained channel, constructing efficient encoders/decoders, and providing the best options for the composite letters to obtain capacity-approaching codes. For certain codes' parameters, our methods incur only one redundant symbol.

2025: Inferring single-cell and spatial microRNA activity from transcriptomics data
Abstract: None

2024: Efficient Random Sampling from Very Large Databases
Abstract: None

2024: A Cyclic Permutation Approach to Removing Spatial Dependency between Clustered Gene Ontology Terms
Abstract: Simple Summary In the intricate field of genomic research, researchers frequently look for the enrichment of genes with a common function. Traditionally, genes are analyzed as if they function independently. However, this assumption may not hold true in large genomic regions, where genes with similar functions exist in close proximity and may influence each other. Our research introduces an advanced method to discern whether the observed patterns in gene groups are due to their spatial closeness, or stem from other biological factors. This approach is particularly crucial in studying large genomic loci, where conventional methods might overlook the nuanced interplay of functionally similar genes. By implementing our technique, we significantly enhance the precision of genomic analyses, particularly in these extensive areas. This advancement is vital as it deepens our understanding of gene interactions within large genomic regions. Abstract Traditional gene set enrichment analysis falters when applied to large genomic domains, where neighboring genes often share functions. This spatial dependency creates misleading enrichments, mistaking mere physical proximity for genuine biological connections. Here we present Spatial Adjusted Gene Ontology (SAGO), a novel cyclic permutation-based approach, to tackle this challenge. SAGO separates enrichments due to spatial proximity from genuine biological links by incorporating the genes’ spatial arrangement into the analysis. We applied SAGO to various datasets in which the identified genomic intervals are large, including replication timing domains, large H3K9me3 and H3K27me3 domains, HiC compartments and lamina-associated domains (LADs). Intriguingly, applying SAGO to prostate cancer samples with large copy number alteration (CNA) domains eliminated most of the enriched GO terms, thus helping to accurately identify biologically relevant gene sets linked to oncogenic processes, free from spatial bias.

2024: Sequence Design and Reconstruction Under the Repeat Channel in Enzymatic DNA Synthesis
Abstract: Using synthetic DNA for data storage and for physical information encoding in labeling, tracing, and authentication applications is becoming more feasible as synthesis and reading technologies are improving. DNA in data storage applications has several advantages such as very high physical density and robustness. Some of the new synthesis technologies lead to repetition noise, consisting of sticky insertions and deletions in the resulting messages. In this paper, we address reconstruction algorithms for multiple trace communication channels with repetition (sticky insertion and deletion) noise. We prove correctness and analyze failure rates, both analytically and on simulated data. We identify a failure mechanism related to alternating stretches in the design sequence that leads to a potential bias in the data derived from reads (traces) and used for reconstruction. To minimize this effect we introduce alternating length limited codes (ALL codes) and analyze some of their properties.

2024: Sequencing coverage analysis for combinatorial DNA-based storage systems
Abstract: This study introduces a novel model for analyzing and determining the required sequencing coverage in DNA-based data storage, focusing on combinatorial DNA encoding. We explore the application of the coupon collector model for combinatorial-letter reconstruction, post-sequencing, which ensure efficient data retrieval and error reduction. We use a Markov Chain model to compute the probability of error-free reconstruction. We develop theoretical bounds on the decoding probability and use empirical simulations to validate these bounds. The work contributes to the understanding of sequencing coverage in DNA-based data storage, offering insights into decoding complexity, error correction, and sequence reconstruction. We provide a Python package that takes the code design and other message parameters as input, and then computes the required read coverage to guarantee reconstruction at a given desired confidence.

2024: Studying the Cycle Complexity of DNA Synthesis
Abstract: Storing data in DNA is being explored as an efficient solution for archiving and in-object storage. Synthesis time and cost remain challenging, significantly limiting some applications at this stage. In this paper we investigate efficient synthesis, as it relates to cyclic synchronized synthesis technologies, such as photolithography. We define performance metrics related to the number of cycles needed for the synthesis of any fixed number of bits. We first expand on some results from the literature related to the channel capacity, addressing densities beyond those covered by prior work. This leads us to develop effective encoding achieving rate and capacity that are higher than previously reported. Finally, we analyze cost based on a parametric definition and determine some bounds and asymptotics. We investigate alphabet sizes that can be larger than 4, both for theoretical completeness and since practical approaches to such schemes were recently suggested and tested in the literature.

2024: Quantifying allele-specific CRISPR editing activity with CRISPECTOR2.0
Abstract: Abstract Off-target effects present a significant impediment to the safe and efficient use of CRISPR-Cas genome editing. Since off-target activity is influenced by the genomic sequence, the presence of sequence variants leads to varying on- and off-target profiles among different alleles or individuals. However, a reliable tool that quantifies genome editing activity in an allelic context is not available. Here, we introduce CRISPECTOR2.0, an extended version of our previously published software tool CRISPECTOR, with an allele-specific editing activity quantification option. CRISPECTOR2.0 enables reference-free, allele-aware, precise quantification of on- and off-target activity, by using de novo sample-specific single nucleotide variant (SNV) detection and statistical-based allele-calling algorithms. We demonstrate CRISPECTOR2.0 efficacy in analyzing samples containing multiple alleles and quantifying allele-specific editing activity, using data from diverse cell types, including primary human cells, plants, and an original extensive human cell line database. We identified instances where an SNV induced changes in the protospacer adjacent motif sequence, resulting in allele-specific editing. Intriguingly, differential allelic editing was also observed in regions carrying distal SNVs, hinting at the involvement of additional epigenetic factors. Our findings highlight the importance of allele-specific editing measurement as a milestone in the adaptation of efficient, accurate, and safe personalized genome editing.

2024: Detecting significant expression patterns in single-cell and spatial transcriptomics with a flexible computational approach
Abstract: None

2024: Representing Information on DNA Using Patterns Induced by Enzymatic Labeling
Abstract: Enzymatic DNA labeling is a powerful tool with applications in biochemistry, molecular biology, biotechnology, medical science, and genomic research. This paper contributes to the evolving field of DNA-based data storage by presenting a formal framework for modeling DNA labeling in strings, specifically tailored for data storage purposes. Our approach involves a known DNA molecule as a template for labeling, employing patterns induced by a set of designed labels to represent information. One hypothetical implementation can use CRISPR-Cas9 and gRNA reagents for labeling. Various aspects of the general labeling channel, including fixed-length labels, are explored, and upper bounds on the maximal size of the corresponding codes are given. The study includes the development of an efficient encoder-decoder pair that is proven optimal in terms of maximum code size under specific conditions.

2024: Comparison of lipidomic profiles sampled with electroporation-based biopsy from healthy skin, squamous cell carcinoma, and basal cell carcinoma.
Abstract: Incidence rates of cutaneous squamous cell carcinoma (cSCC) and basal cell carcinoma (BCC) are increasing, while the current diagnostic process is time consuming. We introduce a high-throughput sampling approach, termed e-biopsy, utilizing electroporation-based biopsy for efficient collection of tissue lipids. Our study identified 168 lipids using ultra-performance liquid chromatography and tandem mass spectrometry (UPLC-MS-MS). The e-biopsy technique demonstrated its ability to profile the human skin lipidome. Comparative analysis revealed 27 differentially expressed lipids (p<0.05). The observed trend of lipidomic profiles was low diglycerides in cSCC and BCC, elevated phospholipids in BCC, and increased lyso-phospholipids in cSCC compared to healthy skin samples. These findings contribute to understanding skin cancers and highlight the potential of e-biopsy for lipidomic analysis in skin tissues.

2024: Tighter Bounds on the Information Bottleneck with Application to Deep Learning
Abstract: Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.

2024: Error-Correcting Codes for Combinatorial DNA Composite
Abstract: —Data storage in DNA is developing as a possible solution for archival digital data. Recently, to further increase the potential capacity of DNA-based data storage systems, the combinatorial composite DNA synthesis method was suggested. This approach extends the DNA alphabet by harnessing short DNA fragment reagents, known as shortmers . The shortmers are building blocks of the alphabet symbols, consisting of a fixed number of shortmers. Thus, when information is read, it is possible that one of the shortmers that forms part of the composition of a symbol is missing and therefore the symbol cannot be determined. In this paper, we model this type of error as a type of asymmetric error and propose code constructions that can correct such errors in this setup. We also provide a lower bound on the redundancy of such error-correcting codes and give an explicit encoder and decoder pair for our construction. Our suggested error model is also supported by an analysis of data from actual experiments that produced DNA according to the combinatorial scheme. Lastly, we also provide a statistical evaluation of the probability of observing such error events, as a function of read depth.

2024: HIPI: Spatially resolved multiplexed protein expression inferred from H&E WSIs
Abstract: Solid tumors are characterized by complex interactions between the tumor, the immune system and the microenvironment. These interactions and intra-tumor variations have both diagnostic and prognostic significance and implications. However, quantifying the underlying processes in patient samples requires expensive and complicated molecular experiments. In contrast, H&E staining is typically performed as part of the routine standard process, and is very cheap. Here we present HIPI (H&E Image Interpretation and Protein Expression Inference) for predicting cell marker expression from tumor H&E images. We process paired H&E and CyCIF images taken from serial sections of colorectal cancers to train our model. We show that our model accurately predicts the spatial distribution of several important cell markers, on both held-out tumor regions as well as new tumor samples taken from different patients. Moreover, using only the tissue image morphology, HIPI is able to colocalize the interactions between different cell types, further demonstrating its potential clinical significance.

2024: Generative Topological Networks
Abstract: Generative methods have recently seen significant improvements by generating in a lower-dimensional latent representation of the data. However, many of the generative methods applied in the latent space remain complex and difficult to train. Further, it is not entirely clear why transitioning to a lower-dimensional latent space can improve generative quality. In this work, we introduce a new and simple generative method grounded in topology theory -- Generative Topological Networks (GTNs) -- which also provides insights into why lower-dimensional latent-space representations might be better-suited for data generation. GTNs are simple to train -- they employ a standard supervised learning approach and do not suffer from common generative pitfalls such as mode collapse, posterior collapse or the need to pose constraints on the neural network architecture. We demonstrate the use of GTNs on several datasets, including MNIST, CelebA, CIFAR-10 and the Hands and Palm Images dataset by training GTNs on a lower-dimensional latent representation of the data. We show that GTNs can improve upon VAEs and that they are quick to converge, generating realistic samples in early epochs. Further, we use the topological considerations behind the development of GTNs to offer insights into why generative models may benefit from operating on a lower-dimensional latent space, highlighting the important link between the intrinsic dimension of the data and the dimension in which the data is generated. Particularly, we demonstrate that generating in high dimensional ambient spaces may be a contributing factor to out-of-distribution samples generated by diffusion models. We also highlight other topological properties that are important to consider when using and designing generative models. Our code is available at: https://github.com/alonalj/GTN

2024: Error-Correcting Codes for Combinatorial Composite DNA
Abstract: Data storage in DNA is developing as a possible solution for archival digital data. Recently, to further increase the potential capacity of DNA-based data storage systems, the combinatorial composite DNA synthesis method was suggested. This approach extends the DNA alphabet by harnessing short DNA fragment reagents, known as shortmers. The shortmers are building blocks of the alphabet symbols, each consisting of a fixed number of shortmers. Thus, when information is read, it is possible that one of the shortmers that forms part of the composition of a symbol is missing and therefore the symbol cannot be determined. In this paper, we model this type of error as a type of asymmetric error and propose code constructions that can correct such errors in this setup. We also provide a lower bound on the redundancy of such error-correcting codes and give an explicit encoder and decoder for our construction. Our suggested error model is also supported by an analysis of data from actual experiments that produced DNA according to the combinatorial scheme. Lastly, we also provide a statistical evaluation of the probability of observing such error events, as a function of read depth.

2024: High Information Density and Low Coverage Data Storage in DNA with Efficient Channel Coding Schemes
Abstract: DNA-based data storage has been attracting significant attention due to its extremely high data storage density, low power consumption, and long duration compared to conventional data storage media. Despite the recent advancements in DNA data storage technology, significant challenges remain. In particular, various types of errors can occur during the processes of DNA synthesis, storage, and sequencing, including substitution errors, insertion errors, and deletion errors. Furthermore, the entire oligo may be lost. In this work, we report a DNA-based data storage architecture that incorporates efficient channel coding schemes, including different types of error-correcting codes (ECCs) and constrained codes, for both the inner coding and outer coding for the DNA data storage channel. We also carried out large scale experiments to validate our proposed DNA-based data storage architecture. Specifically, 1.61 and 1.69 MB data were encoded into 30,000 oligos each, with information densities of 1.731 and 1.815, respectively. It has been found that the stored information can be fully recovered without any error at average coverages of 4.5 and 6.0, respectively. This experiment achieved the highest net information density and lowest coverage among existing DNA-based data storage experiments (with standard DNA), with data recovery rates and coverage approaching theoretical optima.

2023: Detecting rare and weak deviations of non-proportional hazard in survival analysis
Abstract: We propose a new method to compare survival data based on Higher Criticism (HC) of P-values obtained from many exact hypergeometric tests. The method can accommodate censorship and is sensitive to moderate differences in some unknown and relatively few time intervals, attaining much better power against such differences than the log-rank test and other tests that are popular under non-proportional hazard alternatives. We demonstrate the usefulness of the HC-based test in detecting rare differences compared to existing tests using simulated data and using actual gene expression data. Additionally, we analyze the asymptotic power of our method under a piece-wise homogeneous exponential decay model with rare and weak departures, describing two groups experiencing failure rates that are usually identical over time except in a few unknown instances in which the second group's failure rate is higher. Under an asymptotic calibration of the model's parameters, the HC-based test's power experiences a phase transition across the plane involving the rarity and intensity parameters that mirrors the phase transition in a two-sample rare and weak normal means setting. In particular, the phase transition curve of our test indicates a larger region in which it is fully powered than the corresponding region of the log-rank test. %The latter attains a phase transition curve that is analogous to a test based on Fisher's combination statistic of the hypergeometric P-values. %To our knowledge, this is the first analysis of a rare and weak signal detection model that involves individually dependent effects in a non-Gaussian setting.

2023: Analysis of Spatial Molecular Data.
Abstract: None

