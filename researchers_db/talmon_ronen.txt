Recent papers for Talmon Ronen:

2025: VTA projections to M1 are essential for reorganization of layer 2-3 network dynamics underlying motor learning
Abstract: None

2025: Coupled Hierarchical Structure Learning using Tree-Wasserstein Distance
Abstract: In many applications, both data samples and features have underlying hierarchical structures. However, existing methods for learning these latent structures typically focus on either samples or features, ignoring possible coupling between them. In this paper, we introduce a coupled hierarchical structure learning method using tree-Wasserstein distance (TWD). Our method jointly computes TWDs for samples and features, representing their latent hierarchies as trees. We propose an iterative, unsupervised procedure to build these sample and feature trees based on diffusion geometry, hyperbolic geometry, and wavelet filters. We show that this iterative procedure converges and empirically improves the quality of the constructed trees. The method is also computationally efficient and scales well in high-dimensional settings. Our method can be seamlessly integrated with hyperbolic graph convolutional networks (HGCN). We demonstrate that our method outperforms competing approaches in sparse approximation and unsupervised Wasserstein distance learning on several word-document and single-cell RNA-sequencing datasets. In addition, integrating our method into HGCN enhances performance in link prediction and node classification tasks.

2024: Riemannian Covariance Fitting for Direction-of-Arrival Estimation
Abstract: Covariance fitting (CF) is a comprehensive approach for direction of arrival (DoA) estimation, consolidating many common solutions. Standard practice is to use Euclidean criteria for CF, disregarding the intrinsic Hermitian positive-definite (HPD) geometry of the spatial covariance matrices. We assert that this oversight leads to inherent limitations. In this paper, as a remedy, we present a comprehensive study of the use of various Riemannian metrics of HPD matrices in CF. We focus on the advantages of the Affine-Invariant (AI) and the Log-Euclidean (LE) Riemannian metrics. Consequently, we propose a new practical beamformer based on the LE metric and derive analytically its spatial characteristics, such as the beamwidth and sidelobe attenuation, under noisy conditions. Comparing these features to classical beamformers shows significant advantage. In addition, we demonstrate, both theoretically and experimentally, the LE beamformer's robustness in scenarios with small sample sizes and in the presence of noise, interference, and multipath channels.

2024: The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank
Abstract: Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein–Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss.

2024: Direct Position Determination by Covariance-Fitting on the Riemannian Manifold of Hermitian Positive Definite Matrices
Abstract: Direct Position Determination (DPD) is the state-of-the-art solution for emitter localization using multiple phased arrays. This paper shows that DPD can be recast as a covariance-fitting (CF) problem that minimizes the Euclidean distance between a sample covariance matrix ${\mathbf{\hat R}}$ and its location-dependent model R. By showing equivalence to existing DPD methods, this CF viewpoint highlights that the geometry of the Hermitian Positive Definite (HPD) covariance matrices R and ${\mathbf{\hat R}}$ is simply overlooked. Based on this critical observation, we propose a new CF approach for DPD that specifically exploits the Riemannian geometry of HPD matrices for measuring the distance between R and ${\mathbf{\hat R}}$. Experimental results showcase that the proposed Riemannian CF approach for DPD leads to a significant improvement in localization accuracy.

2024: Uncovering avalanche sources via acceleration measurements
Abstract: None

2024: Hyperbolic Diffusion Procrustes Analysis for Intrinsic Representation of Hierarchical Data Sets
Abstract: In this paper, we present Hyperbolic Diffusion Procrustes Analysis (HDPA), a new method for informative representation of hierarchical datasets based on hyperbolic geometry, diffusion geometry, and Procrustes analysis. Our method jointly embeds multiple datasets in a product manifold of hyperbolic spaces, where the data's hidden common hierarchical structure is provably recovered. In addition, our method generates an intrinsic embedding that accommodates the joint representation of multiple datasets with different features, acquired by different equipment, at different sites, or under different environmental conditions. Experimental results demonstrate the efficacy of HDPA on three biomedical datasets comprising heterogeneous gene expression and mass cytometry data.

2024: On learning what to learn: Heterogeneous observations of dynamics and establishing possibly causal relations among them
Abstract: Abstract Before we attempt to (approximately) learn a function between two sets of observables of a physical process, we must first decide what the inputs and outputs of the desired function are going to be. Here we demonstrate two distinct, data-driven ways of first deciding “the right quantities” to relate through such a function, and then proceeding to learn it. This is accomplished by first processing simultaneous heterogeneous data streams (ensembles of time series) from observations of a physical system: records of multiple observation processes of the system. We determine (i) what subsets of observables are common between the observation processes (and therefore observable from each other, relatable through a function); and (ii) what information is unrelated to these common observables, therefore particular to each observation process, and not contributing to the desired function. Any data-driven technique can subsequently be used to learn the input–output relation—from k-nearest neighbors and Geometric Harmonics to Gaussian Processes and Neural Networks. Two particular “twists” of the approach are discussed. The first has to do with the identifiability of particular quantities of interest from the measurements. We now construct mappings from a single set of observations from one process to entire level sets of measurements of the second process, consistent with this single set. The second attempts to relate our framework to a form of causality: if one of the observation processes measures “now,” while the second observation process measures “in the future,” the function to be learned among what is common across observation processes constitutes a dynamical model for the system evolution.

2024: Domain Adaptation for DoA Estimation in Multipath Channels with Interferences
Abstract: We consider the problem of estimating the direction-of-arrival (DoA) of a desired source located in a known region of interest in the presence of interfering sources and multipath. We propose an approach that precedes the DoA estimation and relies on generating a set of reference steering vectors. The steering vectors' generative model is a free space model, which is beneficial for many DoA estimation algorithms. The set of reference steering vectors is then used to compute a function that maps the received signals from the adverse environment to a reference domain free from interfering sources and multipath. We show theoretically and empirically that the proposed map, which is analogous to domain adaption, improves DoA estimation by mitigating interference and multipath effects. Specifically, we demonstrate a substantial improvement in accuracy when the proposed approach is applied before three commonly used beamformers: the delay-and-sum (DS), the minimum variance distortionless response (MVDR), and the Multiple Signal Classification (MUSIC).

2024: Supervised Domain Adaptation Based on Marginal and Conditional Distributions Alignment
Abstract: Supervised domain adaptation (SDA) is an area of machine learning, where the goal is to achieve good generalization performance on data from a target domain, given a small corpus of labeled training data from the target domain and a large corpus of labeled data from a related source domain. In this work, based on a generalization of a well-known theoretical result of Ben-David et al. (2010), we propose an SDA approach, in which the adaptation is performed by aligning the marginal and conditional components of the input-label joint distributions. In addition to being theoretically grounded, we demonstrate that the proposed approach has two advantages over existing SDA approaches. First, it applies to a broad collection of learning tasks, such as regression, classification, multi-label classification, and few-shot learning. Second, it takes into account the geometric structure of the input and label spaces. Experimentally, despite its generality, our approach demonstrates on-par or superior results compared with recent state-of-the-art task-specific methods. Our code is available here.

2024: Multimodal manifold learning using kernel interpolation along geodesic paths
Abstract: None

2024: Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters
Abstract: Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.

2024: Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy
Abstract: Finding meaningful distances between high-dimensional data samples is an important scientific task. To this end, we propose a new tree-Wasserstein distance (TWD) for high-dimensional data with two key aspects. First, our TWD is specifically designed for data with a latent feature hierarchy, i.e., the features lie in a hierarchical space, in contrast to the usual focus on embedding samples in hyperbolic space. Second, while the conventional use of TWD is to speed up the computation of the Wasserstein distance, we use its inherent tree as a means to learn the latent feature hierarchy. The key idea of our method is to embed the features into a multi-scale hyperbolic space using diffusion geometry and then present a new tree decoding method by establishing analogies between the hyperbolic embedding and trees. We show that our TWD computed based on data observations provably recovers the TWD defined with the latent feature hierarchy and that its computation is efficient and scalable. We showcase the usefulness of the proposed TWD in applications to word-document and single-cell RNA-sequencing datasets, demonstrating its advantages over existing TWDs and methods based on pre-trained models.

2024: Parent–child couples display shared neural fingerprints while listening to stories
Abstract: None

2024: SORBET: Automated cell-neighborhood analysis of spatial transcriptomics or proteomics for interpretable sample classification via GNN
Abstract: Spatially resolved transcriptomics or proteomics data have the potential to contribute fundamental insights into the mechanisms underlying physiologic and pathological processes. However, analysis of these data capable of relating spatial information, multiplexed markers, and their observed phenotypes remains technically challenging. To analyze these relationships, we developed SORBET, a deep learning framework that leverages recent advances in graph neural networks (GNN). We apply SORBET to predict tissue phenotypes, such as response to immunotherapy, across different disease processes and different technologies including both spatial proteomics and transcriptomics methods. Our results show that SORBET accurately learns biologically meaningful relationships across distinct tissue structures and data acquisition methods. Furthermore, we demonstrate that SORBET facilitates understanding of the spatially-resolved biological mechanisms underlying the inferred phenotypes. In sum, our method facilitates mapping between the rich spatial and marker information acquired from spatial ‘omics technologies to emergent biological phenotypes. Moreover, we provide novel techniques for identifying the biological processes that comprise the predicted phenotypes.

2024: Landmark Alternating Diffusion
Abstract: Alternating Diffusion (AD) is a commonly applied diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the landmark diffusion idea considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.

2023: Procrustes Analysis on the Manifold of SPSD Matrices for Data Sets Alignment
Abstract: In contemporary high-dimensional data analysis, intrinsically similar and related data sets are often significantly different due to various undesired factors that could arise from different acquisition equipment, calibration, environmental conditions, and many other sources of batch effects. Therefore, the task of aligning such data sets has become ubiquitous. In this work, we present a method for the alignment of different, but related, sets of Symmetric Positive Semidefinite (SPSD) matrices, which constitute a commonly-used family of features, e.g., covariance and correlation matrices, various kernels, and prototypical graph and network representations. Our method does not require any a-priori correspondence, and it is based on non-Euclidean Procrustes Analysis (PA) using a particular Riemannian geometry of SPSD matrices. While the derivation is focused on the manifold of SPSD matrices, we show that our alignment method can be applied directly in the original high-dimensional data space, when considering SPSD features that are sample covariance matrices. We demonstrate the advantage of our approach over competing methods in simulations and in an application to Brain-Computer Interface (BCI) with electroencephalographic (EEG) recordings.

2023: Domain and Modality Adaptation Using Multi-Kernel Matching
Abstract: In this paper, we propose a new method for domain and modality adaptation using multi-kernel matching. Our method is based on the representation of the source and target sets with several local kernels centered at a small number of apriori known corresponding samples. We propose to match the local kernels and, in turn, aggregate the local matches and find a mapping between the source and target sets. We showcase the applicability of our method on simulations and real-world data sets that include EEG recordings for mental arithmetic identification and single-cell multi-omics. In these applications, we demonstrate the advantages of our method over recent competing schemes.

2023: Graph signal interpolation and extrapolation over manifold of Gaussian mixture
Abstract: None

2023: On Interference-Rejection Using Riemannian Geometry for Direction of Arrival Estimation
Abstract: We consider the problem of estimating the direction of arrival of desired acoustic sources in the presence of multiple acoustic interference sources. All the sources are located in noisy and reverberant environments and are received by a microphone array. We propose a new approach for designing beamformers and DoA estimation methods based on the Riemannian geometry of the manifold of Hermitian positive definite matrices. Specifically, we show theoretically that incorporating the Riemannian mean of the spatial correlation matrices into frequently-used beamformers gives rise to spatial spectra that reject the directions of interference sources and result in a higher signal-to-interference ratio. We experimentally demonstrate the advantages of our approach in designing several beamformers and a recent DoA estimation method in the presence of simultaneously active multiple interference sources.

