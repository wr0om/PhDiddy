Recent papers for Seffi Naor:

2024: Approximations and Hardness of Packing Partially Ordered Items
Abstract: Motivated by applications in production planning and storage allocation in hierarchical databases, we initiate the study of covering partially ordered items (CPO). Given a capacity $k \in \mathbb{Z}^+$, and a directed graph $G=(V,E)$ where each vertex has a size in $\{0,1, \ldots,k\}$, we seek a collection of subsets of vertices $S_1, \ldots, S_m$ that cover all the vertices, such that for any $1 \leq j \leq m$, the total size of vertices in $S_j$ is bounded by $k$, and there are no edges from $V \setminus S_j$ to $S_j$. The objective is to minimize the number of subsets $m$. CPO is closely related to the rule caching problem (RCP) that is of wide interest in the networking area. The input for RCP is a directed graph $G=(V,E)$, a profit function $p:V \rightarrow \mathbb{Z}_{0}^+$, and $k \in \mathbb{Z}^+$. The output is a subset $S \subseteq V$ of maximum profit such that $|S| \leq k$ and there are no edges from $V \setminus S$ to $S$. Our main result is a $2$-approximation algorithm for CPO on out-trees, complemented by an asymptotic $1.5$-hardness of approximation result. We also give a two-way reduction between RCP and the densest $k$-subhypergraph problem, surprisingly showing that the problems are equivalent w.r.t. polynomial-time approximation within any factor $\rho \geq 1$. This implies that RCP cannot be approximated within factor $|V|^{1-\eps}$ for any fixed $\eps>0$, under standard complexity assumptions. Prior to this work, RCP was just known to be strongly NP-hard. We further show that there is no EPTAS for the special case of RCP where the profits are uniform, assuming Gap-ETH. Since this variant admits a PTAS, we essentially resolve the complexity status of this problem.

2022: Offline and Online Algorithms for SSD Management
Abstract: The abundance of system-level optimizations for reducing SSD write amplification, which are usually based on experimental evaluation, stands in contrast to the lack of theoretical algorithmic results in this problem domain. To bridge this gap, we explore the problem of reducing write amplification from an algorithmic perspective, considering it in both offline and online settings. In the offline setting, we present a near-optimal algorithm. In the online setting, we first consider algorithms that have no prior knowledge about the input. We present a worst case lower bound and show that the greedy algorithm is optimal in this setting. Then we design an online algorithm that uses predictions about the input. We show that when predictions are pretty accurate, our algorithm circumvents the above lower bound. We complement our theoretical findings with an empirical evaluation of our algorithms, comparing them with the state-of-the-art scheme. The results confirm that our algorithms exhibit an improved performance for a wide range of input traces.

2022: Competitive Algorithms for Block-Aware Caching
Abstract: Motivated by the design of real system storage hierarchies, we study the block-aware caching problem, a generalization of classic caching in which fetching (or evicting) pages from the same block incurs the same cost as fetching (or evicting) just one page from the block. Given a cache of size k, and a sequence of requests from n pages partitioned into given blocks of size β ≤ k, the goal is to minimize the total cost of fetching to (or evicting from) cache. This problem captures generalized caching as a special case, which is already NP-hard offline. We show the following suite of results: For the eviction cost model, we show an O(log k)-approximate offline algorithm, a k-competitive deterministic online algorithm, and an O(log2 k)-competitive randomized online algorithm. For the fetching cost model, we show an integrality gap of Ω(β) for the natural LP relaxation of the problem, and an Ω(β +log k) lower bound for randomized online algorithms. The strategy of ignoring the block-structure and running a classical paging algorithm trivially achieves an O(β) approximation and an O(β log k) competitive ratio respectively for the offline and online-randomized setting. For both fetching and eviction models, we show improved bounds for the (h, k)-bicriteria version of the problem. In particular, when k = 2h, we match the performance of classical caching algorithms up to constant factors. Our results establish a strong separation between the tractability of the fetching and eviction cost models, which is interesting since fetching/eviction costs are the same up to an additive term for the classic caching problem. Previous work of Beckmann et al. (SPAA 21) only studied online deterministic algorithms for the fetching cost model when k > h. Our insight is to relax the block-aware caching problem to a submodular covering linear program. The main technical challenge is to maintain a competitive fractional solution to this LP, and to round it with bounded loss, as the constraints of this LP are revealed online. We hope that this framework is useful going forward for other problems that can be captured as submodular cover.

2021: Recent Advances in Competitive Analysis of Online Algorithms
Abstract: None

2021: An almost optimal approximation algorithm for monotone submodular multiple knapsack
Abstract: None

2021: Online Virtual Machine Allocation with Lifetime and Load Predictions
Abstract: The cloud computing industry has grown rapidly over the last decade, and with this growth there is a significant increase in demand for compute resources. Demand is manifested in the form of Virtual Machine (VM) requests, which need to be assigned to physical machines in a way that minimizes resource fragmentation and efficiently utilizes the available machines. This problem can be modeled as a dynamic version of the bin packing problem with the objective of minimizing the total usage time of the bins (physical machines). Motivated by advances in Machine Learning that provide good estimates of workload characteristics, this paper studies the effect of having extra information about future (total) demand. We show that the competitive factor can be dramatically improved with this additional information; in some cases, we achieve constant competitiveness, or even a competitive factor that approaches 1. Along the way, we design new offline algorithms with improved approximation ratios for the dynamic bin-packing problem.

2021: A Randomness Threshold for Online Bipartite Matching, via Lossless Online Rounding
Abstract: Over three decades ago, Karp, Vazirani and Vazirani (STOC’90) introduced the online bi-partite matching problem. They observed that deterministic algorithms’ competitive ratio for this problem is no greater than 1 / 2 , and proved that randomized algorithms can do better. A natural question thus arises: how random is random ? i.e., how much randomness is needed to outperform deterministic algorithms? The ranking algorithm of Karp et al. requires ˜ O ( n ) random bits, which, ignoring polylog terms, remained unimproved. On the other hand, Pena and Borodin (TCS’19) established a lower bound of (1 − o (1)) log log n random bits for any 1 / 2 + Ω(1) competitive ratio. We close this doubly-exponential gap, proving that, surprisingly, this lower bound is tight. In fact, we prove a sharp threshold of (1 ± o (1)) log log n random bits for the randomness necessary and suﬃcient to outperform deterministic algorithms for this problem, as well as its vertex-weighted generalization. Our work implies the same threshold for the advice complexity (nondeterminism) of these problems. Similar to recent breakthrough results in the online matching literature, for the edge-weighted matching problem (Fahrbach et al. FOCS’20) and adwords without the small bids assumption (Huang et al. FOCS’20), our algorithms break the barrier of 1 / 2 by randomizing matching choices over two neighbors. Unlike these works, our approach does not rely on the recently-introduced OCS machinery, nor the more established randomized primal-dual method. Instead, our work revisits a highly-successful online design technique, which was nonetheless under-utilized in the area of online matching, namely (lossless) online rounding of fractional algorithms. While this technique is known to be hopeless for online matching in general, we show that it is nonetheless applicable to carefully designed fractional algorithms with additional (non-convex) constraints.

2021: Lossless Online Rounding for Online Bipartite Matching (Despite its Impossibility)
Abstract: For numerous online bipartite matching problems, such as edge-weighted matching and matching under two-sided vertex arrivals, the state-of-the-art fractional algorithms outperform their randomized integral counterparts. This gap is surprising, given that the bipartite fractional matching polytope is integral, and so lossless rounding is possible. This gap was explained by Devanur et al.~(SODA'13), who showed that \emph{online} lossless rounding is impossible. Despite the above, we initiate the study of lossless online rounding for online bipartite matching problems. Our key observation is that while lossless online rounding is impossible \emph{in general}, randomized algorithms induce fractional algorithms of the same competitive ratio which by definition are losslessly roundable online. This motivates the addition of constraints that decrease the ``online integrality gap'', thus allowing for lossless online rounding. We characterize a set of non-convex constraints which allow for such lossless online rounding, and better competitive ratios than yielded by deterministic algorithms. As applications of our lossless online rounding approach, we obtain two results of independent interest: (i) a doubly-exponential improvement, and a sharp threshold for the amount of randomness (or advice) needed to outperform deterministic online (vertex-weighted) bipartite matching algorithms, and (ii) an optimal semi-OCS, matching a recent result of Gao et al.~(FOCS'21) answering a question of Fahrbach et al.~(FOCS'20).

2021: Efficient Online Weighted Multi-Level Paging
Abstract: We study the writeback-aware caching problem, a variant of classic paging where paging requests that modify data and requests that leave data intact are treated differently. We give an O(łog^2 k) competitive randomized algorithm, answering an open question of Beckmann ηl~BGHM20 and Even et al. (21) about the existence of a randomized poly-logarithmic competitive algorithm. Our algorithm also works for arbitrary page weights. We also give an O(k) competitive deterministic algorithm, extending the previous result of Beckmann et al. BGHM20 to the weighted setting. Interestingly, we also show that any polynomial-time randomized algorithm must be Ø(łog^2 k)-competitive, assuming NP BPP, based on a connection to online set-cover and using ideas of Feige and Korman (24). This gives a surprising separation from the classical paging problem, where several tight O(łog k)-competitive algorithms are known. A key underlying observation is that writeback-aware caching is algorithmically equivalent to Read/Write (RW) paging, which is an interesting problem on its own and has also been studied previously. We consider a further generalization of RW-paging to multi-level paging, where a page can have l different types (RW-paging corresponds to l=2), and give O(k)-competitive deterministic and O(łog^2 k)-competitive polynomial-time randomized algorithms, without any dependence on l. Our randomized algorithms are based on first finding an O(łog k)-competitive fractional solution to an online linear problem that has additional complicating constraints beyond the usual covering or packing type. Then, we round this fractional solution online losing an additional O(łog k) factor. For l=1, which corresponds to the well-studied weighted paging, our approach gives a substantially simpler and natural rounding algorithm compared to the previous approaches, which might be of independent interest, albeit at the loss of an additional O(łog k) factor.

2021: General Knapsack Problems in a Dynamic Setting
Abstract: The world is dynamic and changes over time, thus any optimization problem used to model real life problems must address this dynamic nature, taking into account the cost of changes to a solution over time. The multistage model was introduced with this goal in mind. In this model we are given a series of instances of an optimization problem, corresponding to different times, and a solution is provided for each instance. The strive for obtaining near-optimal solutions for each instance on one hand, while maintaining similar solutions for consecutive time units on the other hand, is quantified and integrated into the objective function. In this paper we consider the Generalized Multistage $d$-Knapsack problem, a generalization of the multistage variants of the Multiple Knapsack problem, as well as the $d$-Dimensional Knapsack problem. We present a PTAS for Generalized Multistage $d$-Knapsack.

2020: NFV Placement in Resource-Scarce Edge Nodes
Abstract: Multi-access Edge Computing (MEC) is a new networking paradigm considered to be one of the enablers of 5G networks. In particular, it allows for network operators to provide low latency services by moving the service logic from centralized datacenters to small distributed locations at the edge of a network. However, computing and storage resources at these edge nodes are scarce and thus efficient resource allocation becomes an essential building block in any MEC orchestration solution.In this paper we address one particular challenge in this domain - how to place network functions at the edge nodes in a way that maximizes the customers benefit. Thus, we formulate the Capacitated MEC Allocation Problem (CMAP) and provide multiple algorithms with analytical performance guarantees for this problem. Furthermore, we use extensive simulations to evaluate the performance of our algorithms in realistic scenarios and show that they outperform both the analytical worst case guarantees, as well as currently used network function placement methods.

2020: Online k-taxi via Double Coverage and time-reverse primal-dual
Abstract: None

2020: A $(1-e^{-1}-\varepsilon)$-Approximation for the Monotone Submodular Multiple Knapsack Problem
Abstract: We study the problem of maximizing a monotone submodular function subject to a Multiple Knapsack constraint (SMKP) . The input is a set $I$ of items, each associated with a non-negative weight, and a set of bins, each having a capacity. Also, we are given a submodular, monotone and non-negative function $f$ over subsets of the items. The objective is to find a subset of items $A \subseteq I$ and a packing of the items in the bins, such that $f(A)$ is maximized. SMKP is a natural extension of both Multiple Knapsack and the problem of monotone submodular maximization subject to a knapsack constraint. 
Our main result is a nearly optimal polynomial time $(1-e^{-1}-\varepsilon)$-approximation algorithm for the problem, for any $\varepsilon>0$. Our algorithm relies on a refined analysis of techniques for constrained submodular optimization combined with sophisticated application of tools used in the development of approximation schemes for packing problems.

2020: Online Virtual Machine Allocation with Predictions
Abstract: The cloud computing industry has grown rapidly over the last decade, and with this growth there is a significant increase in demand for compute resources. Demand is manifested in the form of Virtual Machine (VM) requests, which need to be assigned to physical machines in a way that minimizes resource fragmentation and efficiently utilizes the available machines. This problem can be modeled as a dynamic version of the bin packing problem with the objective of minimizing the total usage time of the bins (physical machines). Earlier works on dynamic bin packing assumed that no knowledge is available to the scheduler and later works studied models in which lifetime/duration of each "item" (VM in our context) is available to the scheduler. This extra information was shown to improve exponentially the achievable competitive ratio. 
Motivated by advances in Machine Learning that provide good estimates of workload characteristics, this paper studies the effect of having extra information regarding future (total) demand. In the cloud context, since demand is an aggregate over many VM requests, it can be predicted with high accuracy (e.g., using historical data). We show that the competitive factor can be dramatically improved by using this additional information; in some cases, we achieve constant competitiveness, or even a competitive factor that approaches 1. Along the way, we design new offline algorithms with improved approximation ratios for the dynamic bin-packing problem.

2019: Models and Algorithms for Distributed Order Management
Abstract: ,

2019: An Algorithmic Framework for Geo-Distributed Analytics
Abstract: None

2019: Tight Bounds for Online Weighted Tree Augmentation
Abstract: None

2018: k-Servers with a Smile: Online Algorithms via Projections
Abstract: We consider the $k$-server problem on trees and HSTs. We give an algorithms based on %the convex-programming primitive of Bregman projections. This algorithm has a competitive ratios that match some of the recent results given by Bubeck et al. (STOC 2018), whose algorithm was based on mirror-descent-based continuous dynamics prescribed via a differential inclusion.

2018: Netco: Cache and I/O Management for Analytics over Disaggregated Stores
Abstract: We consider a common setting where storage is disaggregated from the compute in data-parallel systems. Colocating caching tiers with the compute machines can reduce load on the interconnect but doing so leads to new resource management challenges. We design a system Netco, which prefetches data into the cache (based on workload predictability), and appropriately divides the cache space and network bandwidth between the prefetches and serving ongoing jobs. Netco makes various decisions (what content to cache, when to cache and how to apportion bandwidth) to support end-to-end optimization goals such as maximizing the number of jobs that meet their service-level objectives (e.g., deadlines). Our implementation of these ideas is available within the open-source Apache HDFS project. Experiments on a public cloud, with production-trace inspired workloads, show that Netco uses up to 5x less remote I/O compared to existing techniques and increases the number of jobs that meet their deadlines up to 80%.

2018: Latency Aware Placement in Multi-access Edge Computing
Abstract: Multi-access Edge Computing (MEC) is a new network architecture that allows applications and network services to be executed at the edge of the network. This is done by running these services on commodity servers that are placed in close proximity to the network edge and to the cellular base stations in wireless networks. This architecture provides high bandwidth and low latency for network functions and other applications. However, the availability of the resources at the network edge is limited and thus one of the main challenges in deploying this new paradigm is the ability to locate these latency sensitive services in the appropriate network location according to the specific demand for each service and the relevant latency constraints. In this paper we address this challenge by defining the Virtual Network Functions Placement and Assignment Problem (VNFPAP) and providing algorithms with guaranteed performance for it. We also show by simulating our algorithms on real mobile data that in realistic scenarios they perform much better than current used heuristics.

