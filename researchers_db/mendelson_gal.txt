Recent papers for Mendelson Gal:

2022: Load Balancing Using Sparse Communication
Abstract: Load balancing across parallel servers is an important class of congestion control problems that arises in service systems. An effective load balancer relies heavily on accurate, real-time congestion information to make routing decisions. However, obtaining such information can impose significant communication overheads, especially in demanding applications like those found in modern data centers. We introduce a framework for communication-aware load balancing and design new load balancing algorithms that perform exceptionally well even in scenarios with sparse communication patterns. Central to our approach is state approximation, where the load balancer first estimates server states through a communication protocol. Subsequently, it utilizes these approximate states within a load balancing algorithm to determine routing decisions. We demonstrate that by using a novel communication protocol, one can achieve accurate queue length approximation with sparse communication: for a maximal approximation error of x, the communication frequency only needs to be O(1/x^2). We further show, via a diffusion analysis, that a constant maximal approximation error is sufficient for achieving asymptotically optimal performance. Taken together, these results therefore demonstrate that highly performant load balancing is possible with very little communication. Through simulations, we observe that the proposed designs match or surpass the performance of state-of-the-art load balancing algorithms while drastically reducing communication rates by up to 90%.

2021: Load balancing with JET: just enough tracking for connection consistency
Abstract: Hash-based stateful load-balancers employ connection tracking to avoid per-connection-consistency (PCC) violations that lead to broken connections. In this paper, we propose Just Enough Tracking (JET), a new algorithmic framework that significantly reduces the size of the connection tracking tables for hash-based stateful load-balancers without increasing PCC violations. Under mild assumptions on how backend servers are added, JET adapts consistent hash techniques to identify which connections do not need to be tracked. We provide a model to identify these safe connections and a pluggable framework with appealing theoretical guarantees that supports a variety of consistent hash and connection-tracking modules. We implement JET in two different environments and with four different consistent hash techniques. Using a series of evaluations, we demonstrate that JET requires connection-tracking tables that are an order of magnitude smaller than those required with full connection tracking while preserving PCC and balance properties. In addition, JET often increases the lookup rate due to improved caching.

2021: EDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning
Abstract: Distributed Mean Estimation (DME) is a central building block in federated learning, where clients send local gradients to a parameter server for av-eraging and updating the model. Due to communication constraints, clients often use lossy compression techniques to compress the gradients, resulting in estimation inaccuracies. DME is more challenging when clients have diverse network conditions, such as constrained communication budgets and packet losses. In such settings, DME techniques often incur a sig-niﬁcant increase in the estimation error leading to degraded learning performance. In this work, we propose a robust DME technique named EDEN that naturally handles heterogeneous communication budgets and packet losses. We derive appealing theoretical guarantees for EDEN and evaluate it empirically. Our results demonstrate that EDEN consistently improves over state-of-the-art DME techniques.

2021: On the Persistent-Idle Load Distribution Policy Under Batch Arrivals and Random Service Capacity
Abstract: Practical considerations of present technological uses of load-balancing policies include the capability to function under server heterogeneity and the need to reduce communication overhead associated with information about the queue states. Under such constraints, state-of-the-art loadbalancing approaches either do not cover the full stability region or provide poor performance. To address this challenge, the Persistent-Idle (PI) policy was recently introduced, where servers only update the dispatcher when they become idle, and the dispatcher always sends jobs to the last server that reported being idle. This policy was proved to achieve the stability region and to operate with low communication overhead (in a sense made precise) for Bernoulli arrivals and heterogeneous servers under the assumption that their service capacities are deterministic and constant. Aiming at a broader range of models that are relevant in applications, we consider in this paper batch arrivals and service capacities that vary over time according to stochastic processes, that may have distinct statistical characteristics across servers. Our main theoretical contribution is to show that the stability region is achieved by this policy in the case of two servers. Then, using extensive simulations, the performance is evaluated for a larger number of heterogeneous servers. PI always appears to be stable and achieves good performance while incurring a negligible communication overhead.

2021: Communication-Efficient Federated Learning via Robust Distributed Mean Estimation
Abstract: Federated learning commonly relies on algorithms such as distributed (mini-batch) SGD, where multiple clients compute their gradients and send them to a central coordinator for averaging and updating the model. To optimize the transmission time and the scalability of the training process, clients often use lossy compression to reduce the message sizes. DRIVE [1] is a recent state of the art algorithm that compresses gradients using one bit per coordinate (with some lower-order overhead). In this technical report, we generalize DRIVE to support any bandwidth constraint as well as extend it to support heterogeneous client resources and make it robust to packet loss

2021: DRIVE: One-bit Distributed Mean Estimation
Abstract: We consider the problem where $n$ clients transmit $d$-dimensional real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques. We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art.

2020: Persistent-Idle Load-Distribution
Abstract: A parallel server system is considered in which a dispatcher routes incoming jobs to a fixed number of heterogeneous servers, each with its own queue. Much effort has been previously made to design policies that use limited state information (e.g., the queue lengths in a small subset of the set of servers, or the identity of the idle servers). However, existing policies either do not achieve the stability region or perform poorly in terms of job completion time. We introduce Persistent-Idle (PI), a new, perhaps counterintuitive, load-distribution policy that is designed to work with limited state information. Roughly speaking, PI always routes to the server that has last been idle. Our main result is that this policy achieves the stability region. Because it operates quite differently from existing policies, our proof method differs from standard arguments in the literature. Specifically, large time properties of reflected random walk, along with a careful choice of a Lyapunov function, are combined to obtain a Lyapunov condition over sufficiently long-time intervals. We also provide simulation results that indicate that job completion times under PI are low for different choices of system parameters, compared with several state-of-the-art load-distribution schemes.

2020: A Lower Bound on the stability region of Redundancy-d with FIFO service discipline
Abstract: None

2019: Replicate to the shortest queues
Abstract: None

2019: Subdiffusive Load Balancing in Time-Varying Queueing Systems
Abstract: The degree to which delays or queue lengths equalize under load-balancing algorithms gives a good indication of their performance. Some of the most well-known results in this context are concerned with the asymptotic behavior of the delay or queue length at the diffusion scale under a critical load condition, where arrival and service rates do not vary with time. For example, under the join-the-shortest-queue policy, the queue length deviation process, defined as the difference between the greatest and smallest queue length as it varies over time, is at a smaller scale (subdiffusive) than that of queue lengths (diffusive).

2018: AnchorHash: A Scalable Consistent Hash
Abstract: Consistent hashing is a central building block in many networking applications, such as maintaining connection affinity of TCP flows. However, current consistent hashing solutions do not ensure full consistency under arbitrary changes or scale poorly in terms of memory footprint, update time and key lookup complexity. We present AnchorHash, a scalable and fully-consistent hashing algorithm. AnchorHash achieves high key lookup rate, low memory footprint and low update time. We formally establish its strong theoretical guarantees, and present an advanced implementation with a memory footprint of only a few bytes per resource. Moreover, evaluations indicate that AnchorHash scales on a single core to 100 million resources while still achieving a key lookup rate of more than 15 million keys per second.

2017: Randomized load balancing in heavy tra c
Abstract: We consider three randomized schemes for load balancing among a xed number, N , of resources, and analyze them in the heavy tra c limit. The rst is join the shortest queue, where arrivals are routed to the shortest among d randomly chosen queues, where d ∈ {2, . . . , N}. The second is redundancy routing, where jobs are replicated d times, routed simultaneously to d randomly chosen queues, and all but the rst copy to be admitted into service are canceled. The third model, that we refer to as replicate to the shortest queues, combines the rst two policies, by replicating d times but sending the copies to the shortest d out of the N queues. We show that under the rst two policies, randomized load balancing dramatically a ects the heavy tra c asymptotics. Namely, it gives rise to di usion scale perturbations of the queue length away from zero, where otherwise (i.e., when d = 1) these perturbations are at the uid scale. For all three models, su cient conditions for state space collapse are provided, and di usion limits are established. AMS subject classi cation: 60F170, 60J60, 60K25, 93E20

2017: dRMT: Disaggregated Programmable Switching (Extended Version)
Abstract: . ABSTRACT We present dRMT (disaggregated Reconfigurable Match-Action Table), a new architecture for programmable switches. dRMT overcomes two important restrictions of RMT, the predominant pipeline-based architecture for programmable switches: (1) table memory is local to an RMT pipeline stage, implying that memory not used by one stage cannot be reclaimed by another, and (2) RMT is hard-wired to always sequentially execute matches followed by actions as packets traverse pipeline stages. We show that these restrictions make it difficult to execute programs efficiently on RMT. dRMT resolves both issues by disaggregating the memory and compute resources of a programmable switch. Specifically, dRMT moves table memories out of pipeline stages and into a centralized pool that is accessible through a crossbar. In addition, dRMT replaces RMT’s pipeline stages with a cluster of processors that can execute match and action operations in any order. We show how to schedule a P4 program on dRMT at compile time to guarantee deterministic throughput and latency. We also present a hardware design for dRMT and analyze its feasibility and chip area. Our results show that dRMT can run programs at line rate with fewer processors compared to RMT, and avoids performance cliffs when there are not enough processors to run a program at line rate. dRMT’s hardware design incurs a modest increase in chip area relative to RMT, mainly due to the crossbar.

2017: dRMT: Disaggregated Programmable Switching
Abstract: We present dRMT (disaggregated Reconfigurable Match-Action Table), a new architecture for programmable switches. dRMT overcomes two important restrictions of RMT, the predominant pipeline-based architecture for programmable switches: (1) table memory is local to an RMT pipeline stage, implying that memory not used by one stage cannot be reclaimed by another, and (2) RMT is hardwired to always sequentially execute matches followed by actions as packets traverse pipeline stages. We show that these restrictions make it difficult to execute programs efficiently on RMT. dRMT resolves both issues by disaggregating the memory and compute resources of a programmable switch. Specifically, dRMT moves table memories out of pipeline stages and into a centralized pool that is accessible through a crossbar. In addition, dRMT replaces RMT's pipeline stages with a cluster of processors that can execute match and action operations in any order. We show how to schedule a P4 program on dRMT at compile time to guarantee deterministic throughput and latency. We also present a hardware design for dRMT and analyze its feasibility and chip area. Our results show that dRMT can run programs at line rate with fewer processors compared to RMT, and avoids performance cliffs when there are not enough processors to run a program at line rate. dRMT's hardware design incurs a modest increase in chip area relative to RMT, mainly due to the crossbar.

2016: On the non-Markovian multiclass queue under risk-sensitive cost
Abstract: None

2016: On the non-Markovian multiclass queue under risk-sensitive cost
Abstract: None

