Recent papers for Nader Bshouty:

2024: On One-Sided Testing Affine Subspaces
Abstract: None

2023: Superpolynomial Lower Bounds for Learning Monotone Classes
Abstract: Koch, Strassle, and Tan [SODA 2023], show that, under the randomized exponential time hypothesis, there is no distribution-free PAC-learning algorithm that runs in time $n^{\tilde O(\log\log s)}$ for the classes of $n$-variable size-$s$ DNF, size-$s$ Decision Tree, and $\log s$-Junta by DNF (that returns a DNF hypothesis). Assuming a natural conjecture on the hardness of set cover, they give the lower bound $n^{\Omega(\log s)}$. This matches the best known upper bound for $n$-variable size-$s$ Decision Tree, and $\log s$-Junta. In this paper, we give the same lower bounds for PAC-learning of $n$-variable size-$s$ Monotone DNF, size-$s$ Monotone Decision Tree, and Monotone $\log s$-Junta by~DNF. This solves the open problem proposed by Koch, Strassle, and Tan and subsumes the above results. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time, and can compute the target function on all the points of the support of the distribution in polynomial time.

2023: A Note on Property Testing of the Binary Rank
Abstract: ,

2023: On Property Testing of the Binary Rank
Abstract: Let M be an n × m (0 , 1)-matrix. We define the s -binary rank, denoted as br s ( M ), of M as the minimum integer d such that there exist d monochromatic rectangles covering all the 1-entries in the matrix, with each 1-entry being covered by at most s rectangles. When s = 1, this corresponds to the binary rank, denoted as br( M ), which is well-known in the literature and has many applications. Let R ( M ) and C ( M ) denote the sets of rows and columns of M , respectively. Using the result of Sgall [10], we establish that if M has an s -binary rank at most d , then | R ( M ) | · | C ( M ) | ≤ (cid:0) d ≤ s (cid:1) 2 d , where (cid:0) d ≤ s (cid:1) = P s i =0 (cid:0) di (cid:1) . This bound is tight, meaning that there exists a matrix M ′ with an s -binary rank of d , for which | R ( M ′ ) | · | C ( M ′ ) | = (cid:0) d ≤ s (cid:1) 2 d . Using this result, we present novel one-sided adaptive and non-adaptive testers for (0 , 1)- matrices with an s -binary rank at most d (and exactly d ). These testers require ˜ O (cid:0)(cid:0) d ≤ s (cid:1) 2 d /ϵ (cid:1) and ˜ O (cid:0)(cid:0) d ≤ s (cid:1) 2 d /ϵ 2 (cid:1) queries, respectively. For a fixed s , this improves upon the query complexity of the tester proposed by Parnas et al. in [9] by a factor of ˜Θ(2 d ).

2023: On Detecting Some Defective Items in Group Testing
Abstract: Group testing is an approach aimed at identifying up to $d$ defective items among a total of $n$ elements. This is accomplished by examining subsets to determine if at least one defective item is present. In our study, we focus on the problem of identifying a subset of $\ell\leq d$ defective items. We develop upper and lower bounds on the number of tests required to detect $\ell$ defective items in both the adaptive and non-adaptive settings while considering scenarios where no prior knowledge of $d$ is available, and situations where an estimate of $d$ or at least some non-trivial upper bound on $d$ is available. When no prior knowledge on $d$ is available, we prove a lower bound of $ \Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$ tests in the randomized non-adaptive settings and an upper bound of $O(\ell \log^2 n)$ for the same settings. Furthermore, we demonstrate that any non-adaptive deterministic algorithm must ask $\Theta(n)$ tests, signifying a fundamental limitation in this scenario. For adaptive algorithms, we establish tight bounds in different scenarios. In the deterministic case, we prove a tight bound of $\Theta(\ell\log{(n/\ell)})$. Moreover, in the randomized settings, we derive a tight bound of $\Theta(\ell\log{(n/d)})$. When $d$, or at least some non-trivial estimate of $d$, is known, we prove a tight bound of $\Theta(d\log (n/d))$ for the deterministic non-adaptive settings, and $\Theta(\ell\log(n/d))$ for the randomized non-adaptive settings. In the adaptive case, we present an upper bound of $O(\ell \log (n/\ell))$ for the deterministic settings, and a lower bound of $\Omega(\ell\log(n/d)+\log n)$. Additionally, we establish a tight bound of $\Theta(\ell \log(n/d))$ for the randomized adaptive settings.

2023: Improved Lower Bound for Estimating the Number of Defective Items
Abstract: Let $X$ be a set of items of size $n$ that contains some defective items, denoted by $I$, where $I \subseteq X$. In group testing, a {\it test} refers to a subset of items $Q \subset X$. The outcome of a test is $1$ if $Q$ contains at least one defective item, i.e., $Q\cap I \neq \emptyset$, and $0$ otherwise. We give a novel approach to obtaining lower bounds in non-adaptive randomized group testing. The technique produced lower bounds that are within a factor of $1/{\log\log\stackrel{k}{\cdots}\log n}$ of the existing upper bounds for any constant~$k$. Employing this new method, we can prove the following result. For any fixed constants $k$, any non-adaptive randomized algorithm that, for any set of defective items $I$, with probability at least $2/3$, returns an estimate of the number of defective items $|I|$ to within a constant factor requires at least $$\Omega\left(\frac{\log n}{\log\log\stackrel{k}{\cdots}\log n}\right)$$ tests. Our result almost matches the upper bound of $O(\log n)$ and solves the open problem posed by Damaschke and Sheikh Muhammad [COCOA 2010 and Discrete Math., Alg. and Appl., 2010]. Additionally, it improves upon the lower bound of $\Omega(\log n/\log\log n)$ previously established by Bshouty [ISAAC 2019].

2023: A Tight Lower Bound of Ω(log n) for the Estimation of the Number of Defective Items
Abstract: Let $X$ be a set of items of size $n$ , which may contain some defective items denoted by $I$, where $I \subseteq X$. In group testing, a {\it test} refers to a subset of items $Q \subset X$. The test outcome is $1$ (positive) if $Q$ contains at least one defective item, i.e., $Q\cap I \neq \emptyset$, and $0$ (negative) otherwise. We give a novel approach to obtaining tight lower bounds in non-adaptive randomized group testing. Employing this new method, we can prove the following result. Any non-adaptive randomized algorithm that, for any set of defective items $I$, with probability at least $2/3$, returns an estimate of the number of defective items $|I|$ to within a constant factor requires at least $\Omega({\log n})$ tests. Our result matches the upper bound of $O(\log n)$ and solves the open problem posed by Damaschke and Sheikh Muhammad.

2022: On properties that are non-trivial to test
Abstract: In this note we show that all sets that are neither ﬁnite nor too dense are non-trivial to test in the sense that, for every (cid:15) > 0, distinguishing between strings in the set and strings that are (cid:15) -far from the set requires Ω(1 /(cid:15) ) queries. Speciﬁcally, we show that if, for inﬁnitely many n ’s, the set contains at least one n -bit long string and at most 2 n − Ω( n ) many n -bit strings, then it is non-trivial to test. This note property testing Speciﬁcally, a tester for a set of strings S is explicitly given two parameters, a length parameter n ∈ N and a proximity parameter (cid:15) > 0, as well as query access to an n -bit string x . The tester is required to distinguish the case that x is in S from the case that x is (cid:15) -far from S , where x is (cid:15) -far from S if its Hamming distance from each | x | -bit long string in S is greater than (cid:15) · | x | . (By distinguishing between strings in A and strings in B we mean accepting each string in A with probability at least 2 / 3 and rejecting each string in B with probability at least 2 / 3.)

2022: Almost Optimal Proper Learning and Testing Polynomials
Abstract: None

2022: On Testing Decision Tree
Abstract: In this paper, we study testing decision tree of size and depth that are significantly smaller than the number of attributes n . Our main result addresses the problem of poly( n, 1 /ϵ ) time algorithms with poly( s, 1 /ϵ ) query complexity (independent of n ) that distinguish between functions that are decision trees of size s from functions that are ϵ -far from any decision tree of size ϕ ( s, 1 /ϵ ), for some function ϕ > s . The best known result is the recent one that follows from Blanc, Lange and Tan, [3], that gives ϕ ( s, 1 /ϵ ) = 2 O ((log 3 s ) /ϵ 3 ) . In this paper, we give a new algorithm that achieves ϕ ( s, 1 /ϵ ) = 2 O (log 2 ( s/ϵ )) . Moreover, we study the testability of depth-d decision tree and give a distribution free tester that distinguishes between depth-d decision tree and functions that are ϵ -far from depth-d 2 decision tree.

2022: Non-Adaptive Proper Learning Polynomials
Abstract: We give the first polynomial-time non-adaptive proper learning algorithm of Boolean sparse multivariate polynomial under the uniform distribution. Our algorithm, for s -sparse polynomial over n variables, makes q = ( s/ϵ ) γ ( s,ϵ ) log n queries where 2 . 66 ≤ γ ( s, ϵ ) ≤ 6 . 922 and runs in ˜ O ( n ) · poly ( s, 1 /ϵ ) time. We also show that for any ϵ = 1 /s O (1) any non-adaptive learning algorithm must make at least ( s/ϵ ) Ω(1) log n queries. Therefore, the query complexity of our algorithm is also polynomial in the optimal query complexity and optimal in n .

2021: On Learning and Testing Decision Tree
Abstract: In this paper, we study learning and testing decision tree of size and depth that are significantly smaller than the number of attributes $n$. Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that distinguish between functions that are decision trees of size $s$ from functions that are $\epsilon$-far from any decision tree of size $\phi(s,1/\epsilon)$, for some function $\phi>s$. The best known result is the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$. Moreover, we study the testability of depth-$d$ decision tree and give a {\it distribution free} tester that distinguishes between depth-$d$ decision tree and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In particular, for decision trees of size $s$, the above result holds in the distribution-free model when the tree depth is $O(\log(s/\epsilon))$. We also give other new results in learning and testing of size-$s$ decision trees and depth-$d$ decision trees that follow from results in the literature and some results we prove in this paper.

2020: Lecture Note on LCSSX's Lower Bounds for Non-Adaptive Distribution-free Property Testing
Abstract: In this lecture note we give Liu-Chen-Servedio-Sheng-Xie's (LCSSX) lower bound for property testing in the non-adaptive distribution-free.

2020: Optimal Randomized Group Testing Algorithm to Determine the Number of Defectives
Abstract: We study the problem of determining exactly the number of defective items in an adaptive Group testing by using a minimum number of tests. We improve the existing algorithm and prove a lower bound that shows that the number of tests in our algorithm is optimal up to small additive terms.

2020: An Optimal Tester for $k$-Linear
Abstract: None

2020: Optimal Deterministic Group Testing Algorithms to Estimate the Number of Defectives
Abstract: None

2019: Almost Optimal Testers for Concise Representations
Abstract: We give improved and almost optimal testers for several classes of Boolean functions on $n$ inputs that have concise representation in the uniform and distribution-free model. Classes, such as $k$-junta, $k$-linear functions, $s$-term DNF, $s$-term monotone DNF, $r$-DNF, decision list, $r$-decision list, size-$s$ decision tree, size-$s$ Boolean formula, size-$s$ branching programs, $s$-sparse polynomials over the binary field and function with Fourier degree at most $d$. The method can be extended to several other classes of functions over any domain that can be approximated by functions that have a small number of relevant variables.

2019: Almost optimal distribution-free junta testing
Abstract: We consider the problem of testing whether an unknown n-variable Boolean function is a k-junta in the distribution-free property testing model, where the distance between functions is measured with respect to an arbitrary and unknown probability distribution over {0, 1}n. Chen, Liu, Servedio, Sheng and Xie [35] showed that the distribution-free k-junta testing can be performed, with one-sided error, by an adaptive algorithm that makes Õ(k2)/ϵ queries. In this paper, we give a simple two-sided error adaptive algorithm that makes Õ{k/ϵ) queries.

2019: Lower Bound for Non-Adaptive Estimation of the Number of Defective Items
Abstract: We prove that to estimate within a constant factor the number of defective items in a non-adaptive randomized group testing algorithm we need at least Ω̃(logn) tests. This solves the open problem posed by Damaschke and Sheikh Muhammad in [6, 7]. 2012 ACM Subject Classification Mathematics of computing; Mathematics of computing→ Discrete mathematics; Mathematics of computing → Probabilistic algorithms; Theory of computation → Probabilistic computation

2019: Bounds for the Number of Tests in Non-Adaptive Randomized Algorithms for Group Testing
Abstract: None

