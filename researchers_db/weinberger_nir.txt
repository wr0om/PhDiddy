Recent papers for Weinberger Nir:

2025: BICompFL: Stochastic Federated Learning with Bi-Directional Compression
Abstract: We address the prominent communication bottleneck in federated learning (FL). We specifically consider stochastic FL, in which models or compressed model updates are specified by distributions rather than deterministic parameters. Stochastic FL offers a principled approach to compression, and has been shown to reduce the communication load under perfect downlink transmission from the federator to the clients. However, in practice, both the uplink and downlink communications are constrained. We show that bi-directional compression for stochastic FL has inherent challenges, which we address by introducing BICompFL. Our BICompFL is experimentally shown to reduce the communication cost by an order of magnitude compared to multiple benchmarks, while maintaining state-of-the-art accuracies. Theoretically, we study the communication cost of BICompFL through a new analysis of an importance-sampling based technique, which exposes the interplay between uplink and downlink communication costs.

2025: On Achievable Rates Over Noisy Nanopore Channels
Abstract: In this paper, we consider a recent channel model of a nanopore sequencer proposed by McBain, Viterbo, and Saunderson (2024), termed the noisy nanopore channel (NNC). In essence, an NNC is a noisy duplication channel, whose input source has a specific Markov structure. We present bounds on the channel capacity of selected NNCs, via simple information-theoretic inequalities. In particular, we provide a (tight) lower bound on the capacity of the noiseless NCC and demonstrate that for an NNC with erasure noise, the capacity approaches $1$ for nanopore memories that scale roughly logarithmically in the length of the input sequence.

2025: A Toolbox for Refined Information-Theoretic Analyses
Abstract: None

2025: DeepDIVE: Optimizing Input-Constrained Distributions for Composite DNA Storage via Multinomial Channel
Abstract: We address the challenge of optimizing the capacity-achieving input distribution for a multinomial channel under the constraint of limited input support size, which is a crucial aspect in the design of DNA storage systems. We propose an algorithm that further elaborates the Multidimensional Dynamic Assignment Blahut-Arimoto (M-DAB) algorithm. Our proposed algorithm integrates variational autoencoder for determining the optimal locations of input distribution, into the alternating optimization of the input distribution locations and weights.

2024: Information Rates Over Multi-View Channels
Abstract: We investigate the fundamental limits of reliable communication over multi-view channels, in which the channel output is comprised of a large number of independent noisy views of a transmitted symbol. We consider first the setting of multi-view discrete memoryless channels and then extend our results to general multi-view channels (using multi-letter formulas). We argue that the channel capacity and dispersion of such multi-view channels converge exponentially fast in the number of views to the entropy and varentropy of the input distribution, respectively. We identify the exact rate of convergence as the smallest Chernoff information between two conditional distributions of the output, conditioned on unequal inputs. For the special case of the deletion channel, we compute upper bounds on this Chernoff information. Finally, we present a new channel model we term the Poisson approximation channel — of possible independent interest — whose capacity closely approximates the capacity of the multi-view binary symmetric channel for any fixed number of views.

2024: Characterization of the Distortion-Perception Tradeoff for Finite Channels with Arbitrary Metrics
Abstract: Whenever inspected by humans, reconstructed signals should not be distinguished from real ones. Typically, such a high perceptual quality comes at the price of high reconstruction error, and vice versa. We study this distortion-perception (DP) tradeoff over finite-alphabet channels, for the Wasserstein-l distance induced by a general metric as the perception index, and an arbitrary distortion matrix. Under this setting, we show that computing the DP function and the optimal reconstructions is equivalent to solving a set of linear programming problems. We provide a structural characterization of the DP tradeoff, where the DP function is piecewise linear in the perception index. We further derive a closed-form expression for the case of binary sources.

2024: Capacity of Frequency-based Channels: Encoding Information in Molecular Concentrations
Abstract: We consider a molecular channel, in which messages are encoded to the frequency of objects (or concentration of molecules) in a pool, and whose output during reading time is a noisy version of the input frequencies, as obtained by sampling with replacement from the pool. We tightly characterize the capacity of this channel using upper and lower bounds, when the number of objects in the pool of objects is constrained. We apply this result to the DNA storage channel in the short-molecule regime, and show that even though the capacity of this channel is technically zero, it can still achieve a large information density.

2024: A Toolbox for Refined Information-Theoretic Analyses with Applications
Abstract: This monograph offers a toolbox of mathematical techniques, which have been effective and widely applicable in information-theoretic analysis. The first tool is a generalization of the method of types to Gaussian settings, and then to general exponential families. The second tool is Laplace and saddle-point integration, which allow to refine the results of the method of types, and are capable of obtaining more precise results. The third is the type class enumeration method, a principled method to evaluate the exact random-coding exponent of coded systems, which results in the best known exponent in various problem settings. The fourth subset of tools aimed at evaluating the expectation of non-linear functions of random variables, either via integral representations, or by a refinement of Jensen's inequality via change-of-measure, by complementing Jensen's inequality with a reversed inequality, or by a class of generalized Jensen's inequalities that are applicable for functions beyond convex/concave. Various application examples of all these tools are provided along this monograph.

2024: Information Rates Over DMCs with Many Independent Views
Abstract: In this paper, we investigate the fundamental limits of reliable communication over a discrete memoryless channel (DMC) when there are a large number of noisy views of a transmitted symbol, i.e., when several copies of a single symbol are sent independently through the DMC. We argue that the channel capacity and dispersion of such a multi-view DMC converge exponentially quickly in the number of views to to the entropy and varentropy of the input distribution, respectively, and identify the exact rate of convergence. This rate equals the smallest Chernoff information between two conditional distributions of the output given unequal inputs. Our results hence help us characterize the largest finite-blocklength rates achievable for any fixed error probability. We also present a new channel model that we call the Poisson approximation channel-of possible independent interest-whose capacity closely approximates the capacity of the multi-view binary symmetric channel (BSC).

2024: Capacity-Maximizing Input Symbol Selection for Discrete Memoryless Channels
Abstract: Motivated by communication systems with constrained complexity, we consider the problem of input symbol selection for discrete memoryless channels (DMCs). Given a DMC, the goal is to find a subset of its input alphabet, so that the optimal input distribution that is only supported on these symbols maximizes the capacity among all other subsets of the same size (or smaller). We observe that the resulting optimization problem is non-concave and non-submodular, and so generic methods for such cases do not have theoretical guarantees. We derive an analytical upper bound on the capacity loss when selecting a subset of input symbols based only on the properties of the transition matrix of the channel. We propose a selection algorithm that is based on input-symbols clustering, and an appropriate choice of representatives for each cluster, which uses the theoretical bound as a surrogate objective function. We provide numerical experiments to support the findings.

2024: Maximal-Capacity Discrete Memoryless Channel Identification
Abstract: The problem of identifying the channel with the highest capacity among several discrete memoryless channels (DMCs) is considered. The problem is cast as a pure-exploration multi-armed bandit problem, which follows the practical use of training sequences to sense the communication channel statistics. A gap-elimination algorithm termed BestChanID is proposed, which is oblivious to the capacity-achieving input distributions, and is guaranteed to output the DMC with the largest capacity, with a desired confidence. Furthermore, two additional algorithms NaiveChanSel and MedianChanEl, which output with certain confidence a DMC with capacity close to the maximal, are also presented. Each of these algorithms is shown to be beneficial in a different regime and can be used as a subroutine of BestChanID. To analyze the algorithms’ guarantees, a capacity estimator is proposed and tight confidence bounds on the estimator error are derived. Based on this estimator, the sample complexity of all the proposed algorithms is analyzed as a function of the desired confidence parameter, the number of channels, and the channels’ input and output alphabet sizes. The cost of best channel identification is shown to scale quadratically with the alphabet size, and a fundamental lower bound is derived on the number of channel senses required to identify the best channel with a certain confidence.

2024: On Bits and Bandits: Quantifying the Regret-Information Trade-off
Abstract: In many sequential decision problems, an agent performs a repeated task. He then suffers regret and obtains information that he may use in the following rounds. However, sometimes the agent may also obtain information and avoid suffering regret by querying external sources. We study the trade-off between the information an agent accumulates and the regret it suffers. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. We also prove regret upper bounds using the amount of information the agent accumulates. These bounds show that information measured in bits, can be traded off for regret, measured in reward. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.

2024: Fundamental Limits of Reference-Based Sequence Reordering
Abstract: We consider the problem of reconstructing a sequence of independent and identically distributed symbols from a set of equal-size, consecutive, fragments, as well as a dependent reference sequence. First, in the regime in which the fragments are relatively long and typically no fragment appears more than once, we determine the scaling of the failure probability of the maximum-likelihood reconstruction algorithm for a perfect reconstruction, and bound it for a partial reconstruction. Second, we characterize the regime in which the fragments are relatively short and repeating fragments abound. We state a trade-off between the fraction of fragments that cannot be adequately reconstructed vs. the distortion level allowed for the reconstruction of each fragment, while still allowing vanishing failure probability.

2023: M-DAB: An Input-Distribution Optimization Algorithm for Composite DNA Storage by the Multinomial Channel
Abstract: Recent experiments have shown that the capacity of DNA storage systems may be significantly increased by synthesizing composite DNA letters. In this work, we model a DNA storage channel with composite inputs as a \textit{multinomial channel}, and propose an optimization algorithm for its capacity achieving input distribution, for an arbitrary number of output reads. The algorithm is termed multidimensional dynamic assignment Blahut-Arimoto (M-DAB), and is a generalized version of the DAB algorithm, proposed by Wesel et al. developed for the binomial channel. We also empirically observe a scaling law behavior of the capacity as a function of the support size of the capacity-achieving input distribution.

2023: Maximal-Capacity Discrete Memoryless Channel Identification
Abstract: We consider the problem of finding the channel with the highest capacity among several discrete memoryless channels (DMCs) with the same input-output alphabet sizes by means of exploration using multi-armed bandits. This setting is motivated by the problem of exploring channel statistics in communication systems by the invocation of training sequences. We particularly focus on the best arm identification problem and rank the candidate DMCs by their capacities. We propose a capacity estimator based on channel sensing and derive associated concentration results. Using this capacity estimator, we introduce BestChanID, a gap-elimination algorithm, oblivious to the capacity-achieving input distribution, which is guaranteed to output the best DMC, i.e., DMC with the largest capacity, with a desired confidence. We further introduce NaiveChanSel, an algorithm that outputs with certain confidence a DMC whose capacity is close to the largest capacity, and can be used as a subroutine in BestChanID. We analyze the sample complexity of both algorithms, i.e., the total number of channel senses, as a function of the desired confidence parameter, the number of available channels, and the input and output alphabet sizes of the channels. We show that the cost of best channel identification scales cubically with the alphabet size.

2023: Fundamental Limits of Reference-Based Sequence Reordering
Abstract: The problem of reconstructing a sequence of independent and identically distributed symbols from a set of equal size, consecutive, fragments, as well as a dependent reference sequence is considered. First, in the regime in which the fragments are relatively long, and typically no fragment appears more than once, the scaling of the failure probability of maximum likelihood reconstruction algorithm is exactly determined for perfect reconstruction and bounded for partial reconstruction. Second, the regime in which the fragments are relatively short and repeating fragments abound is characterized. A trade-off is stated between the fraction of fragments that fail to be adequately reconstructed vs. the distortion level allowed for the reconstruction of each fragment, while still allowing vanishing failure probability.

2023: Design of optimal labeling patterns for optical genome mapping via information theory
Abstract: Optical genome mapping (OGM) is a technique that extracts partial genomic information from optically imaged and linearized DNA fragments containing fluorescently labeled short sequence patterns. This information can be used for various genomic analyses and applications, such as the detection of structural variations and copy-number variations, epigenomic profiling, and microbial species identification. Currently, the choice of labeled patterns is based on the available bio-chemical methods, and is not necessarily optimized for the application. In this work, we develop a model of OGM based on information theory, which enables the design of optimal labeling patterns for specific applications and target organism genomes. We validated the model through experimental OGM on human DNA and simulations on bacterial DNA. Our model predicts up to 10-fold improved accuracy by optimal choice of labeling patterns, which may guide future development of OGM bio-chemical labeling methods and significantly improve its accuracy and yield for applications such as epigenomic profiling and cultivation-free pathogen identification in clinical samples.

2023: On Mismatched Oblivious Relaying
Abstract: We consider the problem of reliable communication over a discrete memoryless channel (DMC) with the help of a relay, termed the information bottleneck (IB) channel. There is no direct link between the source and the destination, and the information flows in two hops. The first hop is a noisy channel from the source to the relay. The second hop is a noiseless but limited-capacity backhaul link from the relay to the decoder. We further assume that the relay is oblivious to the transmission codebook. We examine two mismatch scenarios. In the first setting, we assume the decoder is restricted to use some fixed decoding rule, which is mismatched to the actual channel. In the second setting, we assume that the relay is restricted to use some fixed compression metric, which is again mismatched to the statistics of the relay input. We establish bounds on the random-coding capacity of both settings, some of which are shown to be ensemble tight.

2022: Learning Maximum Margin Channel Decoders for Non-linear Gaussian Channels
Abstract: The problem of learning a channel decoder for an unknown non-linear white Gaussian noise channel is considered. The learner is provided with a fixed codebook and a dataset comprised of n independent input-output samples of the channel, and is required to select a matrix for a nearest neighbor decoder with a linear kernel. The objective of maximizing the margin of the decoder is addressed. Accordingly, a regularized loss minimization problem with a codebook-related regularization term and a hinge-like loss function is developed, which is inspired by the support vector machine paradigm for classification problems. Expected generalization error bound for that hinge loss is provided for the solution of the regularized loss minimization, and shown to scale at a rate of O(1/(λn)), where λ is a regularization tradeoff parameter. In addition, a high probability uniform generalization error bound is provided for the hypothesis class, and shown to scale at a rate of $O(1/\sqrt n )$. A stochastic sub-gradient descent algorithm for solving the regularized loss minimization problem is proposed, and an optimization error bound is stated, which scales at a rate of $\tilde O(1/(\lambda T))$. The performance of the this algorithm is demonstrated by an example.

2022: The DNA Storage Channel: Capacity and Error Probability Bounds
Abstract: The DNA storage channel is considered, in which the <inline-formula> <tex-math notation="LaTeX">$M$ </tex-math></inline-formula> Deoxyribonucleic acid (DNA) molecules comprising each codeword are stored without order, sampled <inline-formula> <tex-math notation="LaTeX">$N$ </tex-math></inline-formula> times with replacement, and then sequenced over a discrete memoryless channel. For a constant coverage depth <inline-formula> <tex-math notation="LaTeX">$M/N$ </tex-math></inline-formula> and molecule length scaling <inline-formula> <tex-math notation="LaTeX">$\Theta (\log M)$ </tex-math></inline-formula>, lower (achievability) and upper (converse) bounds on the capacity of the channel, as well as a lower (achievability) bound on the reliability function of the channel are provided. Both the lower and upper bounds on the capacity generalize a bound which was previously known to hold only for the binary symmetric sequencing channel, and only under certain restrictions on the molecule length scaling and the crossover probability parameters. When specified to binary symmetric sequencing channel, these restrictions are completely removed for the lower bound and are significantly relaxed for the upper bound in the high-noise regime. The lower bound on the reliability function is achieved under a universal decoder, and reveals that the dominant error event is that of <italic>outage</italic> – the event in which the capacity of the channel induced by the DNA molecule sampling operation does not support the target rate.

